{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "datastore = 'azureml://datastores/workspaceblobstore/paths/'\n",
        "submissions_path = 'filtered-submissions'\n",
        "submissions_df = spark.read.parquet(f\"{datastore}{submissions_path}\")\n",
        "\n",
        "# take a subset of columns\n",
        "df = submissions_df.select(\"subreddit\", \"author\", \"title\", \"selftext\",\n",
        "                             \"created_utc\", \"num_comments\", \"score\", \n",
        "                             \"over_18\", \"media\", \"pinned\", \"locked\", \n",
        "                             \"disable_comments\", \"domain\", \"hidden\", \n",
        "                             \"distinguished\", \"hide_score\")\n",
        "\n",
        "from pyspark.sql.functions import length\n",
        "\n",
        "# Assuming your DataFrame is named `df`\n",
        "df = df.withColumn('post_length', length(df.title) + length(df.selftext))\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = df.withColumn('created_utc', F.to_timestamp('created_utc'))\n",
        "\n",
        "# Extract time-based features\n",
        "df = df.withColumn('hour_of_day', F.hour('created_utc'))\n",
        "df = df.withColumn('day_of_week', F.dayofweek('created_utc'))  # 1 (Sunday) to 7 (Saturday)\n",
        "# Map each day of the week from numeric to string\n",
        "df = df.withColumn('day_of_week_str', F.expr(\"\"\"\n",
        "    CASE day_of_week \n",
        "        WHEN 1 THEN 'Sunday'\n",
        "        WHEN 2 THEN 'Monday'\n",
        "        WHEN 3 THEN 'Tuesday'\n",
        "        WHEN 4 THEN 'Wednesday'\n",
        "        WHEN 5 THEN 'Thursday'\n",
        "        WHEN 6 THEN 'Friday'\n",
        "        WHEN 7 THEN 'Saturday'\n",
        "    END\n",
        "\"\"\"))\n",
        "df = df.withColumn('day_of_month', F.dayofmonth('created_utc'))\n",
        "df = df.withColumn('month', F.month('created_utc'))\n",
        "df = df.withColumn('year', F.year('created_utc'))\n",
        "\n",
        "df = df.withColumn('has_media', F.col('media').isNotNull())\n",
        "\n",
        "df = df.drop(*[\"media\", \"disable_comments\", \"distinguished\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "107",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T00:15:12.9484087Z",
              "session_start_time": "2023-11-13T00:15:13.0015671Z",
              "execution_start_time": "2023-11-13T00:16:04.8164794Z",
              "execution_finish_time": "2023-11-13T00:16:13.0433059Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 6:\ndatastore = 'azureml://datastores/workspaceblobstore/paths/'\nsubmissions_path = 'filtered-submissions'\nsubmissions_df = spark.read.parquet(f\"{datastore}{submissions_path}\")\n\n# take a subset of columns\ndf = submissions_df.select(\"subreddit\", \"author\", \"title\", \"selftext\",\n                             \"created_utc\", \"num_comments\", \"score\", \n                             \"over_18\", \"media\", \"pinned\", \"locked\", \n                             \"disable_comments\", \"domain\", \"hidden\", \n                             \"distinguished\", \"hide_score\")\n\nfrom pyspark.sql.functions import length\n\n# Assuming your DataFrame is named `df`\ndf = df.withColumn('post_length', length(df.title) + length(df.selftext))\n\nfrom pyspark.sql import functions as F\n\ndf = df.withColumn('created_utc', F.to_timestamp('created_utc'))\n\n# Extract time-based features\ndf = df.withColumn('hour_of_day', F.hour('created_utc'))\ndf = df.withColumn('day_of_week', F.dayofweek('created_utc'))  # 1 (Sunday) to 7 (Saturday)\n# Map each day of the week from numeric...",
                    "submissionTime": "2023-11-13T00:16:07.022GMT",
                    "completionTime": "2023-11-13T00:16:10.526GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "6",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "760e7fca-5e4c-4f1e-a63a-7e8091d5c328"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699834573140
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PLOT_DIR = os.path.join(\"Users/st1140/fall-2023-reddit-project-team-34/data\", \"plots\")\n",
        "CSV_DIR = os.path.join(\"Users/st1140/fall-2023-reddit-project-team-34/data\", \"csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "106",
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T23:54:52.0633237Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T23:54:52.190417Z",
              "execution_finish_time": "2023-11-12T23:54:52.489711Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4769cd0c-b9be-488c-aa0e-a7292d71fd02"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 106, 19, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699833292558
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "97",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:37:30.2175334Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:37:30.3559882Z",
              "execution_finish_time": "2023-11-12T03:37:30.6646577Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "23d2cad2-a79b-4705-8d8b-a40dafe45c3e"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 97, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- subreddit: string (nullable = true)\n |-- author: string (nullable = true)\n |-- title: string (nullable = true)\n |-- selftext: string (nullable = true)\n |-- created_utc: timestamp (nullable = true)\n |-- num_comments: long (nullable = true)\n |-- score: long (nullable = true)\n |-- over_18: boolean (nullable = true)\n |-- pinned: boolean (nullable = true)\n |-- locked: boolean (nullable = true)\n |-- domain: string (nullable = true)\n |-- hidden: boolean (nullable = true)\n |-- hide_score: boolean (nullable = true)\n |-- post_length: integer (nullable = true)\n |-- hour_of_day: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- day_of_week_str: string (nullable = true)\n |-- day_of_month: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- year: integer (nullable = true)\n |-- has_media: boolean (nullable = false)\n\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699760250851
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table for Top comments of all sub reddits"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
        "filtered_df = df.filter(df.subreddit.isin('movies', 'anime', 'television'))\n",
        "\n",
        "# Select columns for Plotly DataFrame\n",
        "df_plotly = filtered_df.select([\"subreddit\", \"title\",\"num_comments\", \"selftext\",\"author\",\"score\"])\n",
        "\n",
        "# Order the DataFrame in descending order based on the num_comments column\n",
        "df_plotly_sorted = df_plotly.orderBy(desc(\"num_comments\"))\n",
        "\n",
        "# Retrieve the top 20 rows\n",
        "df_top_20 = df_plotly_sorted.limit(20)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "98",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:50:17.0947208Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:50:17.2498361Z",
              "execution_finish_time": "2023-11-12T03:50:18.0429887Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "2efc5695-5d14-47e2-849b-d02ed8fc57e9"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 98, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699761018132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd=df_top_20.toPandas()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "97",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:37:51.480767Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:37:51.6841891Z",
              "execution_finish_time": "2023-11-12T03:39:00.8248333Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_18002/2722203675.py:1",
                    "dataWritten": 847531,
                    "dataRead": 927859369,
                    "rowCount": 1020968,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "toPandas at /tmp/ipykernel_18002/2722203675.py:1",
                    "description": "Job group for statement 17:\ndf_top_20_pd=df_top_20.toPandas()",
                    "submissionTime": "2023-11-12T03:37:51.849GMT",
                    "completionTime": "2023-11-12T03:38:58.817GMT",
                    "stageIds": [
                      5,
                      6
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 101,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 101,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 12,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 101,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f8909676-56c0-4a72-affa-7dd9666872e8"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 97, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699760341038
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd.to_csv(f\"{CSV_DIR}/top_comments_eda_1.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "97",
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:39:05.6798892Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:39:05.8505653Z",
              "execution_finish_time": "2023-11-12T03:39:07.3568649Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "131bb0b2-fd55-4a6d-aab4-6090d60708ab"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 97, 18, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699760347450
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of posts by Authors with Top Comments"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataFrame is named 'df' and has a column named 'author'\n",
        "authors_to_count = [\"dpemerson76\", \"lionsgate\", \"LiteraryBoner\",\"AnimeMod\",\"prsnreddit\",\"officialtobeymaguire\",\"KillerQ97\",\"leedavis1987\",\"Jeff_Souza\"]\n",
        "\n",
        "# Filter the DataFrame based on the specified authors\n",
        "filtered_df = df.filter(col(\"author\").isin(authors_to_count))\n",
        "\n",
        "# Count the occurrences of each author\n",
        "author_counts = filtered_df.groupBy(\"author\").count()\n",
        "\n",
        "# Show the result\n",
        "author_counts.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "98",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:50:30.2345537Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:50:30.3557883Z",
              "execution_finish_time": "2023-11-12T03:51:43.8901022Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 23812,
                    "rowCount": 309,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\n# Assuming your DataFrame is named 'df' and has a column named 'author'\nauthors_to_count = [\"dpemerson76\", \"lionsgate\", \"LiteraryBoner\",\"AnimeMod\",\"prsnreddit\",\"officialtobeymaguire\",\"KillerQ97\",\"leedavis1987\",\"Jeff_Souza\"]\n\n# Filter the DataFrame based on the specified authors\nfiltered_df = df.filter(col(\"author\").isin(authors_to_count))\n\n# Count the occurrences of each author\nauthor_counts = filtered_df.groupBy(\"author\").count()\n\n# Show the result\nauthor_counts.show()",
                    "submissionTime": "2023-11-12T03:51:42.906GMT",
                    "completionTime": "2023-11-12T03:51:43.408GMT",
                    "stageIds": [
                      2,
                      3
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 101,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 100,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 23812,
                    "dataRead": 765019597,
                    "rowCount": 372727,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\n# Assuming your DataFrame is named 'df' and has a column named 'author'\nauthors_to_count = [\"dpemerson76\", \"lionsgate\", \"LiteraryBoner\",\"AnimeMod\",\"prsnreddit\",\"officialtobeymaguire\",\"KillerQ97\",\"leedavis1987\",\"Jeff_Souza\"]\n\n# Filter the DataFrame based on the specified authors\nfiltered_df = df.filter(col(\"author\").isin(authors_to_count))\n\n# Count the occurrences of each author\nauthor_counts = filtered_df.groupBy(\"author\").count()\n\n# Show the result\nauthor_counts.show()",
                    "submissionTime": "2023-11-12T03:50:32.103GMT",
                    "completionTime": "2023-11-12T03:51:42.636GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 100,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 100,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 15,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 100,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "16252807-06be-4d25-b34d-95825f3d0490"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 98, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+-----+\n|              author|count|\n+--------------------+-----+\n|       LiteraryBoner|  549|\n|          prsnreddit|  519|\n|            AnimeMod|  755|\n|          Jeff_Souza|    4|\n|           lionsgate|    9|\n|        leedavis1987|    4|\n|officialtobeymaguire|    1|\n|         dpemerson76|    4|\n|           KillerQ97|    8|\n+--------------------+-----+\n\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699761104026
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joinedDF = df_top_20.join(author_counts.select('author', 'count'), ['author'])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "98",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:54:33.6627647Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:54:33.7966409Z",
              "execution_finish_time": "2023-11-12T03:54:34.0962067Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a2f279b3-fe74-45e1-8f4c-512be5488aee"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 98, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699761274185
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 20 movies"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
        "filtered_df_movies = df.filter(df.subreddit.isin('movies'))\n",
        "\n",
        "# Select columns for Plotly DataFrame\n",
        "df_plotly_movies = filtered_df_movies.select([\"subreddit\",\"title\", \"num_comments\", \"selftext\",\"author\",\"score\"])\n",
        "\n",
        "# Order the DataFrame in descending order based on the num_comments column\n",
        "df_plotly_sorted_movies = df_plotly_movies.orderBy(desc(\"num_comments\"))\n",
        "\n",
        "# Retrieve the top 20 rows\n",
        "df_top_20_movies = df_plotly_sorted_movies.limit(20)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "101",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:40:26.2919811Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:40:26.3867188Z",
              "execution_finish_time": "2023-11-12T22:40:27.2152651Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3524b996-647d-424c-8da1-6573a23e6919"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 101, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699828827295
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd_movies=df_top_20_movies.toPandas()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "101",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:40:30.5804917Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:40:30.7040414Z",
              "execution_finish_time": "2023-11-12T22:42:04.0515907Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_7280/739558151.py:1",
                    "dataWritten": 805557,
                    "dataRead": 953349237,
                    "rowCount": 1020968,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "toPandas at /tmp/ipykernel_7280/739558151.py:1",
                    "description": "Job group for statement 9:\ndf_top_20_pd_movies=df_top_20_movies.toPandas()",
                    "submissionTime": "2023-11-12T22:40:38.034GMT",
                    "completionTime": "2023-11-12T22:42:01.478GMT",
                    "stageIds": [
                      1,
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 101,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 101,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 12,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 101,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "8e0a868c-3f63-422e-baa2-794cc8d88b91"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 101, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699828924379
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd_movies.to_csv(f\"{CSV_DIR}/top_comments_movies.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "101",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:44:20.6354055Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:44:20.8020885Z",
              "execution_finish_time": "2023-11-12T22:44:22.4095197Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a8a7bf13-42ab-4828-9d2d-4f10de28979d"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 101, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829062556
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
        "filtered_df_anime = df.filter(df.subreddit.isin('anime'))\n",
        "\n",
        "# Select columns for Plotly DataFrame\n",
        "df_plotly_anime = filtered_df_anime.select([\"subreddit\",\"title\", \"num_comments\", \"selftext\",\"author\",\"score\"])\n",
        "\n",
        "# Order the DataFrame in descending order based on the num_comments column\n",
        "df_plotly_sorted_anime = df_plotly_anime.orderBy(desc(\"num_comments\"))\n",
        "\n",
        "# Retrieve the top 20 rows\n",
        "df_top_20_anime = df_plotly_sorted_anime.limit(20)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "102",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:49:28.7398623Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:49:28.8813191Z",
              "execution_finish_time": "2023-11-12T22:49:31.0176013Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "81e907d0-928e-4a8e-80aa-993d8df0e141"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 102, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829371083
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd_anime = df_top_20_anime.toPandas()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "102",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:49:32.9128286Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:49:33.1009222Z",
              "execution_finish_time": "2023-11-12T22:50:54.8763664Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_21080/3063620563.py:1",
                    "dataWritten": 950788,
                    "dataRead": 953494468,
                    "rowCount": 1020968,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "toPandas at /tmp/ipykernel_21080/3063620563.py:1",
                    "description": "Job group for statement 9:\ndf_top_20_pd_anime = df_top_20_anime.toPandas()",
                    "submissionTime": "2023-11-12T22:49:38.781GMT",
                    "completionTime": "2023-11-12T22:50:53.520GMT",
                    "stageIds": [
                      1,
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 101,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 101,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 12,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 101,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "eba8d0b0-9085-4df2-97eb-15863adcf831"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 102, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829454940
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd_anime.to_csv(f\"{CSV_DIR}/top_comments_anime.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "102",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:51:24.2954386Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:51:24.443555Z",
              "execution_finish_time": "2023-11-12T22:51:26.6999805Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "20ebaf89-4c84-451a-a7e5-01a35bebbf8a"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 102, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829486826
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
        "filtered_df_television = df.filter(df.subreddit.isin('television'))\n",
        "\n",
        "# Select columns for Plotly DataFrame\n",
        "df_plotly_television = filtered_df_television.select([\"subreddit\", \"num_comments\",\"title\", \"selftext\",\"author\",\"score\"])\n",
        "\n",
        "# Order the DataFrame in descending order based on the num_comments column\n",
        "df_plotly_sorted_television = df_plotly_television.orderBy(desc(\"num_comments\"))\n",
        "\n",
        "# Retrieve the top 20 rows\n",
        "df_top_20_television = df_plotly_sorted_television.limit(50)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "104",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:54:08.3493722Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:54:08.5156792Z",
              "execution_finish_time": "2023-11-12T22:54:09.37352Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4eb3fef1-87b8-4941-8e5d-5a67e9eb70f5"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 104, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829649445
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd_television=df_top_20_television.toPandas()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "104",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:54:10.4445065Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:54:10.6433001Z",
              "execution_finish_time": "2023-11-12T22:55:25.8779982Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_23882/1382704528.py:1",
                    "dataWritten": 1043516,
                    "dataRead": 953587196,
                    "rowCount": 1026968,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "toPandas at /tmp/ipykernel_23882/1382704528.py:1",
                    "description": "Job group for statement 9:\ndf_top_20_pd_television=df_top_20_television.toPandas()",
                    "submissionTime": "2023-11-12T22:54:12.486GMT",
                    "completionTime": "2023-11-12T22:55:24.181GMT",
                    "stageIds": [
                      1,
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 101,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 101,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 14,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 101,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "0f46e660-db14-4f55-b0ba-9112cd816c27"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 104, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829725951
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_20_pd_television.to_csv(f\"{CSV_DIR}/top_comments_tv_show.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "104",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T22:56:16.4977417Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T22:56:16.5997269Z",
              "execution_finish_time": "2023-11-12T22:56:18.11008Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "e404d364-cf67-4a60-8755-a6711a043ed3"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 104, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699829778161
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'author' and count the occurrences\n",
        "author_counts = df.groupBy(\"author\").count()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "95",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-12T03:07:17.8244281Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-12T03:07:17.9706556Z",
              "execution_finish_time": "2023-11-12T03:07:18.2632977Z",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "281cb801-6dd6-419b-bffe-8252d3c27268"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 95, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699758438342
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_movies = pd.read_csv(\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/top_comments_movies.csv\")\n",
        "df_anime = pd.read_csv(\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/top_comments_anime.csv\")\n",
        "df_tv = pd.read_csv(\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/top_comments_tv_show.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "107",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T00:17:10.6135506Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T00:17:10.7322195Z",
              "execution_finish_time": "2023-11-13T00:17:12.2119827Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "80a28e7f-8448-4ba7-b139-7548946b8b05"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699834632322
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.concat([df_movies, df_anime, df_tv], axis=0)\n",
        "unique_authors = list(result[[\"subreddit\", \"author\"]].drop_duplicates()[\"author\"].unique())\n",
        "count_of_posts_per_author = df.filter(df.author.isin(unique_authors)).groupBy(\"author\").count().toPandas()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "107",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T00:17:13.8490794Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T00:17:14.035103Z",
              "execution_finish_time": "2023-11-13T00:18:35.4021809Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
                    "dataWritten": 0,
                    "dataRead": 147629,
                    "rowCount": 1897,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
                    "description": "Job group for statement 8:\nresult = pd.concat([df_movies, df_anime, df_tv], axis=0)\nunique_authors = list(result[[\"subreddit\", \"author\"]].drop_duplicates()[\"author\"].unique())\ncount_of_posts_per_author = df.filter(df.author.isin(unique_authors)).groupBy(\"author\").count().toPandas()",
                    "submissionTime": "2023-11-13T00:18:31.647GMT",
                    "completionTime": "2023-11-13T00:18:32.532GMT",
                    "stageIds": [
                      2,
                      3
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 101,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 100,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
                    "dataWritten": 147629,
                    "dataRead": 1134165430,
                    "rowCount": 1018865,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
                    "description": "Job group for statement 8:\nresult = pd.concat([df_movies, df_anime, df_tv], axis=0)\nunique_authors = list(result[[\"subreddit\", \"author\"]].drop_duplicates()[\"author\"].unique())\ncount_of_posts_per_author = df.filter(df.author.isin(unique_authors)).groupBy(\"author\").count().toPandas()",
                    "submissionTime": "2023-11-13T00:17:16.308GMT",
                    "completionTime": "2023-11-13T00:18:31.460GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 100,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 100,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 15,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 100,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4162d4d0-ffd3-4810-a9ae-2b3aa5c5b5ae"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699834715529
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "authors_with_top_comments_post_counts = result.merge(count_of_posts_per_author, how=\"left\", on=\"author\")\n",
        "authors_with_top_comments_post_counts.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "authors_with_top_comments_post_counts = authors_with_top_comments_post_counts[authors_with_top_comments_post_counts['author'] != '[deleted]']\n",
        "authors_with_top_comments_post_counts.to_csv(f\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/authors_with_top_comments_post_counts.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
              "session_id": "107",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T00:18:44.6309336Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T00:18:44.7785651Z",
              "execution_finish_time": "2023-11-13T00:18:45.0762001Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4de98614-68a0-4f08-a0d5-590dece503a1"
            },
            "text/plain": "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699834725167
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}