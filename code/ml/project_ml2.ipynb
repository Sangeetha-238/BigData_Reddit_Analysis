{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286a9e95-5e29-4586-be7c-882441067e05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.11.17         |  py310h06a4308_0         158 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       341.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> pkgs/main::ca-certificates-2023.08.22-h06a4308_0 \n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/linux-64::certifi-2023.11.17-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openjdk-11.0.13      | 341.0 MB  |                                       |   0% \n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "certifi-2023.11.17   | 158 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "ca-certificates-2023 | 123 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spark-nlp==5.1.3\n",
      "  Obtaining dependency information for spark-nlp==5.1.3 from https://files.pythonhosted.org/packages/cd/7d/bc0eca4c9ec4c9c1d9b28c42c2f07942af70980a7d912d0aceebf8db32dd/spark_nlp-5.1.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached spark_nlp-5.1.3-py2.py3-none-any.whl.metadata (53 kB)\n",
      "Using cached spark_nlp-5.1.3-py2.py3-none-any.whl (537 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# install spark-nlp\n",
    "%pip install spark-nlp==5.1.3\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f397c1-0d71-4aaf-93ea-2a2e0eb1e63c",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING - SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b67c6079-aed2-43cf-a5fb-d5dde690e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pyspark.sql.functions import lower, regexp_replace, col, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.sql.functions import length\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9364e386-da55-45bc-802d-03ecc7cfee11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.executor.memory\", \"12g\")\\\n",
    "    .config(\"spark.executor.cores\", \"3\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3,org.apache.hadoop:hadoop-aws:3.2.2\")\\\n",
    "    .config(\n",
    "            \"fs.s3a.aws.credentials.provider\",\n",
    "            \"com.amazonaws.auth.ContainerCredentialsProvider\"\n",
    "    )\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9127d1-904d-4189-837e-3c885d26f039",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df81bfd-4fa1-431d-bddd-8dd79b4531e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "reading submissions from s3a://project-group34/project/submissions/yyyy=*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 00:36:53 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 30 ms, total: 176 ms\n",
      "Wall time: 6.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 00:36:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket = \"project-group34\"\n",
    "session = sagemaker.Session()\n",
    "output_prefix_data_submissions = \"project/submissions/yyyy=*\"\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data_submissions}\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "submissions = spark.read.parquet(s3_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05160e48-a70c-4ebd-bb4f-e5a263641430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named `df`\n",
    "submissions = submissions.withColumn('post_length', length(submissions.title) + length(submissions.selftext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4f37fe-a452-45fd-b178-935e8cd54234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "submissions = submissions.withColumn('created_utc', F.to_timestamp('created_utc'))\n",
    "\n",
    "# Extract time-based features\n",
    "submissions = submissions.withColumn('hour_of_day', F.hour('created_utc'))\n",
    "submissions = submissions.withColumn('day_of_week', F.dayofweek('created_utc'))  # 1 (Sunday) to 7 (Saturday)\n",
    "# Map each day of the week from numeric to string\n",
    "submissions = submissions.withColumn('day_of_week_str', F.expr(\"\"\"\n",
    "    CASE day_of_week \n",
    "        WHEN 1 THEN 'Sunday'\n",
    "        WHEN 2 THEN 'Monday'\n",
    "        WHEN 3 THEN 'Tuesday'\n",
    "        WHEN 4 THEN 'Wednesday'\n",
    "        WHEN 5 THEN 'Thursday'\n",
    "        WHEN 6 THEN 'Friday'\n",
    "        WHEN 7 THEN 'Saturday'\n",
    "    END\n",
    "\"\"\"))\n",
    "submissions = submissions.withColumn('day_of_month', F.dayofmonth('created_utc'))\n",
    "submissions = submissions.withColumn('month', F.month('created_utc'))\n",
    "submissions = submissions.withColumn('year', F.year('created_utc'))\n",
    "\n",
    "submissions = submissions.withColumn('has_media', F.col('media').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "477703c8-4d25-4b81-8d4f-de3662f40c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions = submissions.drop(*[\"media\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d062db5-6ea1-40bd-ab83-3196261a1146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions = submissions.select('subreddit',\n",
    "                                 'title',\n",
    "                                 'selftext',\n",
    "                                 'score',\n",
    "                                 'num_comments',\n",
    "                                 'over_18',\n",
    "                                 'is_self',\n",
    "                                 'is_video',\n",
    "                                 'domain',\n",
    "                                 'post_length',\n",
    "                                 'hour_of_day',\n",
    "                                 'day_of_week',\n",
    "                                 'day_of_week_str',\n",
    "                                 'day_of_month',\n",
    "                                 'month',\n",
    "                                 'year',\n",
    "                                 'has_media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d2219b-8737-46bb-a14d-1c5293e0938b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine 'title' and 'selftext' into a new column 'body'\n",
    "submissions = submissions.withColumn(\"body\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e92dfc-bca1-4f00-8272-2f5a3a81eef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions = submissions.drop(*[\"title\", \"selftext\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60cf2d66-11a5-4f2c-af7b-75fbb27cf1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+-------+-------+--------+---------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+\n",
      "| subreddit|score|num_comments|over_18|is_self|is_video|         domain|post_length|hour_of_day|day_of_week|day_of_week_str|day_of_month|month|year|has_media|                body|\n",
      "+----------+-----+------------+-------+-------+--------+---------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+\n",
      "|television|    0|           9|  false|   true|   false|self.television|        605|         22|          4|      Wednesday|          27|    1|2021|    false|Is there a websit...|\n",
      "|     anime|    0|           3|  false|  false|   false|      i.redd.it|         50|         22|          4|      Wednesday|          27|    1|2021|    false|Does anyone know ...|\n",
      "|television|    4|          11|  false|  false|   false|   deadline.com|         86|         22|          4|      Wednesday|          27|    1|2021|    false|‘Doogie Kameāloha...|\n",
      "|    movies|    0|           4|  false|   true|   false|    self.movies|         42|         22|          4|      Wednesday|          27|    1|2021|    false|4K movies on desk...|\n",
      "|     anime|    0|           9|  false|   true|   false|     self.anime|         64|         22|          4|      Wednesday|          27|    1|2021|    false|Where can I buy a...|\n",
      "+----------+-----+------------+-------+-------+--------+---------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "submissions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5806c-9d0d-413f-ac50-60ddd35c1763",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NLP DATA CLEANING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5b6a3d9-b69a-4f32-84b4-ae9e253e6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Create a DocumentAssembler to convert text into documents\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"body\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Tokenize the document\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# Normalize and set case insensitive to be true\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma')\n",
    "\n",
    "# Define a list of punctuation symbols to remove\n",
    "punctuation_symbols = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"]\n",
    "\n",
    "# Remove punctuation using StopWordsCleaner\n",
    "punctuation_remover = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('cleaned_lemma') \\\n",
    "    .setStopWords(punctuation_symbols)\n",
    "\n",
    "# Finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['cleaned_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d7c608-02c4-4ee0-b370-edb3690849e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           lemmatizer,\n",
    "           punctuation_remover,\n",
    "           finisher\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "357ff83e-485e-4370-9014-22660be28b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.4.0.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(submissions)\n",
    "result = pipelineModel.transform(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c88c706-a23c-42df-a7e6-216bcdbd658d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = result.withColumn(\"final_text\", F.concat_ws(\" \", \"finished_cleaned_lemma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b89251d7-2fb0-4e23-b395-9273388da800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+-------+-------+--------+---------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+\n",
      "| subreddit|score|num_comments|over_18|is_self|is_video|         domain|post_length|hour_of_day|day_of_week|day_of_week_str|day_of_month|month|year|has_media|                body|            document|               token|          normalized|               lemma|       cleaned_lemma|finished_cleaned_lemma|          final_text|\n",
      "+----------+-----+------------+-------+-------+--------+---------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+\n",
      "|television|    0|           9|  false|   true|   false|self.television|        605|         22|          4|      Wednesday|          27|    1|2021|    false|Is there a websit...|[{document, 0, 60...|[{token, 0, 1, Is...|[{token, 0, 1, is...|[{token, 0, 1, be...|[{token, 0, 1, be...|  [be, there, a, we...|be there a websit...|\n",
      "|     anime|    0|           3|  false|  false|   false|      i.redd.it|         50|         22|          4|      Wednesday|          27|    1|2021|    false|Does anyone know ...|[{document, 0, 50...|[{token, 0, 3, Do...|[{token, 0, 3, do...|[{token, 0, 3, do...|[{token, 0, 3, do...|  [do, anyone, know...|do anyone know wh...|\n",
      "|television|    4|          11|  false|  false|   false|   deadline.com|         86|         22|          4|      Wednesday|          27|    1|2021|    false|‘Doogie Kameāloha...|[{document, 0, 86...|[{token, 0, 6, ‘D...|[{token, 0, 5, do...|[{token, 0, 5, do...|[{token, 0, 5, do...|  [doogie, kameāloh...|doogie kameāloha ...|\n",
      "|    movies|    0|           4|  false|   true|   false|    self.movies|         42|         22|          4|      Wednesday|          27|    1|2021|    false|4K movies on desk...|[{document, 0, 42...|[{token, 0, 1, 4K...|[{token, 0, 0, k,...|[{token, 0, 0, k,...|[{token, 0, 0, k,...|  [k, movie, on, de...|k movie on deskto...|\n",
      "|     anime|    0|           9|  false|   true|   false|     self.anime|         64|         22|          4|      Wednesday|          27|    1|2021|    false|Where can I buy a...|[{document, 0, 64...|[{token, 0, 4, Wh...|[{token, 0, 4, wh...|[{token, 0, 4, wh...|[{token, 0, 4, wh...|  [where, can, i, b...|where can i buy a...|\n",
      "+----------+-----+------------+-------+-------+--------+---------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4626c8b-f624-4b52-b8f0-423db46e9771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.write.parquet(\"s3a://project-group34/project/submissions/sentiment_analysis/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289af1d-af40-4529-909a-db1d54411131",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SENTIMENT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c662e-24f1-4927-911b-626b794b4de4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPARK JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1170e6dd-e8a1-4f81-94b6-3212769d0409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/ml_sentiment_submissions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/ml_sentiment_submissions.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType\n",
    "import re\n",
    "from pyspark.sql.functions import explode, count\n",
    "import sagemaker\n",
    "from pyspark.sql.functions import lower, regexp_replace, col, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "eng_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"final_text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "    # Paths to the models\n",
    "    # tfhub_use_path = \"../../../cache_pretrained/tfhub_use_en_2.4.0_2.4_1587136330099/\"\n",
    "    # sentimentdl_use_twitter_path = \"../../../cache_pretrained/sentimentdl_use_twitter_en_2.7.1_2.4_1610983524713/\"\n",
    "    # sentiment_emotion = \"../../../cache_pretrained/cache_pretrained/classifierdl_use_emotion_en_2.7.1_2.4_1610190563302/\"\n",
    "\n",
    "    # Load models from local path\n",
    "    use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    "             .setInputCols([\"document\"])\\\n",
    "             .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "    sentimentdl = SentimentDLModel.pretrained(name=\"sentimentdl_use_twitter\", lang=\"en\")\\\n",
    "                     .setInputCols([\"sentence_embeddings\"])\\\n",
    "                     .setOutputCol(\"sentiment\")\n",
    "\n",
    "    # sentimentdl1 = ClassifierDLModel.load(sentiment_emotion)\\\n",
    "    #     .setInputCols([\"sentence_embeddings\"])\\\n",
    "    #     .setOutputCol(\"sentiment_emotion\")\n",
    "\n",
    "    nlpPipeline = Pipeline(\n",
    "          stages = [\n",
    "              documentAssembler,\n",
    "              use,\n",
    "              sentimentdl\n",
    "              #sentimentdl1\n",
    "          ])\n",
    "\n",
    "    # Apply the pipeline to your DataFrame\n",
    "    model = nlpPipeline.fit(df)\n",
    "    result = model.transform(df)\n",
    "    \n",
    "    result.write.parquet(\"s3a://project-group34/project/submissions/sentiment_extracted/\", mode=\"overwrite\")\n",
    "    \n",
    "    logger.info(f\"all done...\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5ad4b16-d96b-47a3-a6a4-5f27ae5d1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 0 ns, total: 14.7 ms\n",
      "Wall time: 33.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09434be-9f4b-4aec-a7e0-c411af8aa5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'655678691473'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af214fde-67c7-4beb-9170-cf26cd5f3e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "CPU times: user 203 ms, sys: 0 ns, total: 203 ms\n",
      "Wall time: 244 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# # S3 URI of the initialization script\n",
    "# s3_uri_init_script = f's3://{bucket}/{script_key}'\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.executor.memory\": \"12g\", \"spark.executor.cores\": \"4\"},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d4bc6fe-f020-4302-b558-a79de1a47cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-11-29-01-51-36-228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................!CPU times: user 620 ms, sys: 48.3 ms, total: 669 ms\n",
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket = \"project-group34\"\n",
    "output_prefix_data_comments = \"project/submissions/sentiment_analysis/\"\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data_comments}\"\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/ml_sentiment_submissions.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_path,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    "    configuration=configuration\n",
    ")\n",
    "# give some time for resources from this iterations to get cleaned up\n",
    "# if we start the job immediately we could get insufficient resources error\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039ed94-86ee-4db5-b5c9-fbda89fafc82",
   "metadata": {},
   "source": [
    "### SENTIMENT MODEL PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92e9a20a-2b93-43fe-804e-8796af37e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pyspark.sql.functions import lower, regexp_replace, col, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e44dc12e-5075-449e-bfd0-ef7449fec359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.executor.memory\", \"12g\")\\\n",
    "    .config(\"spark.executor.cores\", \"3\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3,org.apache.hadoop:hadoop-aws:3.2.2\")\\\n",
    "    .config(\n",
    "            \"fs.s3a.aws.credentials.provider\",\n",
    "            \"com.amazonaws.auth.ContainerCredentialsProvider\"\n",
    "    )\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5474a63f-abc6-430b-947a-391ca8b0253b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = spark.read.parquet(\"s3a://project-group34/project/submissions/sentiment_extracted/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77c9e223-b8aa-480e-b22a-d2c8ec953a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+-------+-------+--------+------------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|subreddit|score|num_comments|over_18|is_self|is_video|            domain|post_length|hour_of_day|day_of_week|day_of_week_str|day_of_month|month|year|has_media|                body|            document|               token|          normalized|               lemma|       cleaned_lemma|finished_cleaned_lemma|          final_text| sentence_embeddings|           sentiment|\n",
      "+---------+-----+------------+-------+-------+--------+------------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|    anime|    0|           2|  false|   true|   false|        self.anime|         62|          3|          4|      Wednesday|           7|   12|2022|    false|Rumor: ONE PUNCH ...|[{document, 0, 56...|[{token, 0, 4, Ru...|[{token, 0, 4, ru...|[{token, 0, 4, ru...|[{token, 0, 4, ru...|  [rumor, one, punc...|rumor one punch m...|[{sentence_embedd...|[{category, 0, 56...|\n",
      "|    anime|    2|           2|  false|  false|   false|mobile.twitter.com|         53|          3|          4|      Wednesday|           7|   12|2022|     true|RUMOR: ONE PUNCH ...|[{document, 0, 49...|[{token, 0, 4, RU...|[{token, 0, 4, ru...|[{token, 0, 4, ru...|[{token, 0, 4, ru...|  [rumor, one, punc...|rumor one punch m...|[{sentence_embedd...|[{category, 0, 49...|\n",
      "|    anime|    1|           1|  false|   true|   false|                  |         46|          3|          4|      Wednesday|           7|   12|2022|    false|Can mappa become ...|[{document, 0, 39...|[{token, 0, 2, Ca...|[{token, 0, 2, ca...|[{token, 0, 2, ca...|[{token, 0, 2, ca...|  [can, mappa, beco...|can mappa become ...|[{sentence_embedd...|[{category, 0, 39...|\n",
      "|    anime|    1|           1|  false|   true|   false|        self.anime|         47|          3|          4|      Wednesday|           7|   12|2022|    false|Can mappa become ...|[{document, 0, 39...|[{token, 0, 2, Ca...|[{token, 0, 2, ca...|[{token, 0, 2, ca...|[{token, 0, 2, ca...|  [can, mappa, beco...|can mappa become ...|[{sentence_embedd...|[{category, 0, 39...|\n",
      "|   movies|    1|           1|  false|   true|   false|       self.movies|         58|          3|          4|      Wednesday|           7|   12|2022|    false|Which actors/actr...|[{document, 0, 49...|[{token, 0, 4, Wh...|[{token, 0, 4, wh...|[{token, 0, 4, wh...|[{token, 0, 4, wh...|  [which, actorsact...|which actorsactre...|[{sentence_embedd...|[{category, 0, 49...|\n",
      "+---------+-----+------------+-------+-------+--------+------------------+-----------+-----------+-----------+---------------+------------+-----+----+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0d9d763-d9ef-4620-8250-0472193d413a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n",
      "|sentiment                                                                                         |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "|[{category, 0, 56, negative, {sentence -> 0, positive -> 0.09468776, negative -> 0.90531224}, []}]|\n",
      "|[{category, 0, 49, positive, {sentence -> 0, positive -> 0.890747, negative -> 0.10925303}, []}]  |\n",
      "|[{category, 0, 39, negative, {sentence -> 0, positive -> 4.9209936E-21, negative -> 1.0}, []}]    |\n",
      "|[{category, 0, 39, negative, {sentence -> 0, positive -> 4.9209936E-21, negative -> 1.0}, []}]    |\n",
      "|[{category, 0, 49, negative, {sentence -> 0, positive -> 0.06990129, negative -> 0.9300988}, []}] |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\"sentiment\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8f06292-ffc1-4200-ba70-eb243119d5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result1 = result.select(\n",
    "    F.expr(\"sentiment[0].metadata['positive']\").cast(\"float\").alias(\"positive_score\"),\n",
    "    col(\"subreddit\"),\n",
    "    col(\"final_text\").alias(\"text\"),\n",
    "    col(\"score\"),\n",
    "    col(\"num_comments\"),\n",
    "    col(\"over_18\"),\n",
    "    col(\"is_self\"),\n",
    "    col(\"is_video\"),\n",
    "    col(\"post_length\"),\n",
    "    col(\"hour_of_day\"),\n",
    "    col(\"day_of_week\"),\n",
    "    col(\"day_of_month\"),\n",
    "    col(\"month\"),\n",
    "    col(\"year\"),\n",
    "    col(\"has_media\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15521e4b-9ea1-4d8f-bab8-0d9dd648c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+\n",
      "|positive_score|subreddit|                text|score|num_comments|over_18|is_self|is_video|post_length|hour_of_day|day_of_week|day_of_month|month|year|has_media|\n",
      "+--------------+---------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+\n",
      "|    0.09468776|    anime|rumor one punch m...|    0|           2|  false|   true|   false|         62|          3|          4|           7|   12|2022|    false|\n",
      "|      0.890747|    anime|rumor one punch m...|    2|           2|  false|  false|   false|         53|          3|          4|           7|   12|2022|     true|\n",
      "| 4.9209936E-21|    anime|can mappa become ...|    1|           1|  false|   true|   false|         46|          3|          4|           7|   12|2022|    false|\n",
      "| 4.9209936E-21|    anime|can mappa become ...|    1|           1|  false|   true|   false|         47|          3|          4|           7|   12|2022|    false|\n",
      "|    0.06990129|   movies|which actorsactre...|    1|           1|  false|   true|   false|         58|          3|          4|           7|   12|2022|    false|\n",
      "+--------------+---------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5a525f8-b18e-4005-8ff9-72f89b246aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result1.write.parquet(\"s3a://project-group34/project/submissions/sentiment_submissions/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce4b01-905d-45f5-9e38-3060d93eec82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EMOTION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5a67f-5258-4777-9c22-90612e9256ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SPARK JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48ac1f87-8f44-4522-87bd-3618b6f7ffca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/ml_emotion_submissions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/ml_emotion_submissions.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType\n",
    "import re\n",
    "from pyspark.sql.functions import explode, count\n",
    "import sagemaker\n",
    "from pyspark.sql.functions import lower, regexp_replace, col, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "eng_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "    # Paths to the models\n",
    "    # tfhub_use_path = \"../../../cache_pretrained/tfhub_use_en_2.4.0_2.4_1587136330099/\"\n",
    "    # sentimentdl_use_twitter_path = \"../../../cache_pretrained/sentimentdl_use_twitter_en_2.7.1_2.4_1610983524713/\"\n",
    "    # sentiment_emotion = \"../../../cache_pretrained/cache_pretrained/classifierdl_use_emotion_en_2.7.1_2.4_1610190563302/\"\n",
    "\n",
    "    # Load models from local path\n",
    "    use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    "             .setInputCols([\"document\"])\\\n",
    "             .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "    # sentimentdl = SentimentDLModel.pretrained(name=\"sentimentdl_use_twitter\", lang=\"en\")\\\n",
    "    #                  .setInputCols([\"sentence_embeddings\"])\\\n",
    "    #                  .setOutputCol(\"sentiment\")\n",
    "\n",
    "    sentimentdl1 = ClassifierDLModel.pretrained(name=\"classifierdl_use_emotion\")\\\n",
    "        .setInputCols([\"sentence_embeddings\"])\\\n",
    "        .setOutputCol(\"sentiment_emotion\")\n",
    "\n",
    "    nlpPipeline = Pipeline(\n",
    "          stages = [\n",
    "              documentAssembler,\n",
    "              use,\n",
    "              sentimentdl1\n",
    "              #sentimentdl\n",
    "          ])\n",
    "\n",
    "    # Apply the pipeline to your DataFrame\n",
    "    model = nlpPipeline.fit(df)\n",
    "    result = model.transform(df)\n",
    "    \n",
    "    result.write.parquet(\"s3a://project-group34/project/submissions/emotion_extracted/\", mode=\"overwrite\")\n",
    "    \n",
    "    logger.info(f\"all done...\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d2709476-2be8-4e35-ae5a-c6eb479959b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 ms, sys: 618 µs, total: 17.3 ms\n",
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80164ef5-f57c-4beb-b965-61bc849d5381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'655678691473'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a203c534-fc61-4c6c-aa0d-1a681af8948a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "CPU times: user 74.2 ms, sys: 3.44 ms, total: 77.7 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# # S3 URI of the initialization script\n",
    "# s3_uri_init_script = f's3://{bucket}/{script_key}'\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.executor.memory\": \"12g\", \"spark.executor.cores\": \"4\"},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66a06333-a6a5-4b80-9b7a-d176a6a8db9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-11-29-03-06-29-254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................................!CPU times: user 681 ms, sys: 60.6 ms, total: 742 ms\n",
      "Wall time: 13min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucket = \"project-group34\"\n",
    "output_prefix_data_comments = \"project/submissions/sentiment_submissions/\"\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data_comments}\"\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/ml_emotion_submissions.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_path,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    "    configuration=configuration\n",
    ")\n",
    "# give some time for resources from this iterations to get cleaned up\n",
    "# if we start the job immediately we could get insufficient resources error\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5f43c-d7a1-42df-86f8-332dffec0fbc",
   "metadata": {},
   "source": [
    "### EMOTION MODEL PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e207a4b-390a-4cad-b424-540f3587156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from pyspark.sql.functions import lower, regexp_replace, col, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbc6afc-4871-48a4-ab99-d0cc82e24ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.executor.memory\", \"12g\")\\\n",
    "    .config(\"spark.executor.cores\", \"3\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3,org.apache.hadoop:hadoop-aws:3.2.2\")\\\n",
    "    .config(\n",
    "            \"fs.s3a.aws.credentials.provider\",\n",
    "            \"com.amazonaws.auth.ContainerCredentialsProvider\"\n",
    "    )\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba10b6b-8dfe-464a-8425-8b3589799c20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 03:21:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.read.parquet(\"s3a://project-group34/project/submissions/emotion_extracted/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b27cc6-3c3a-48d9-8178-ffb5be291c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+--------------------+--------------------+--------------------+\n",
      "|positive_score| subreddit|                text|score|num_comments|over_18|is_self|is_video|post_length|hour_of_day|day_of_week|day_of_month|month|year|has_media|            document| sentence_embeddings|   sentiment_emotion|\n",
      "+--------------+----------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+--------------------+--------------------+--------------------+\n",
      "|           1.0|    movies|sidney poitier a ...|17101|         501|  false|  false|   false|         53|         16|          7|          20|    2|2021|    false|[{document, 0, 48...|[{sentence_embedd...|[{category, 0, 48...|\n",
      "|     0.5907684|     anime|help i get into a...|    0|          13|  false|   true|   false|         63|         16|          7|          20|    2|2021|    false|[{document, 0, 58...|[{sentence_embedd...|[{category, 0, 58...|\n",
      "|     0.9160415|television|wwe elimination c...|    0|           1|  false|  false|   false|        120|         16|          7|          20|    2|2021|    false|[{document, 0, 10...|[{sentence_embedd...|[{category, 0, 10...|\n",
      "|     0.6181543|    movies|chadwick bossman ...|    2|           0|  false|  false|   false|         48|         16|          7|          20|    2|2021|     true|[{document, 0, 43...|[{sentence_embedd...|[{category, 0, 43...|\n",
      "|    0.20858091|    movies|crank and be the ...|  829|         197|  false|   true|   false|       1280|         16|          7|          20|    2|2021|    false|[{document, 0, 11...|[{sentence_embedd...|[{category, 0, 11...|\n",
      "+--------------+----------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30309e2-63e8-432a-97e3-d233e2861966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- positive_score: float (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- is_video: boolean (nullable = true)\n",
      " |-- post_length: integer (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- has_media: boolean (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = true)\n",
      " |    |    |-- end: integer (nullable = true)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |-- sentence_embeddings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = true)\n",
      " |    |    |-- end: integer (nullable = true)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |-- sentiment_emotion: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = true)\n",
      " |    |    |-- end: integer (nullable = true)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0a468c-6ba6-4d0e-a603-099d2390c23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentiment_emotion                                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{category, 0, 48, surprise, {surprise -> 0.94696283, joy -> 0.0021084917, fear -> 0.048108246, sadness -> 0.0028204152, sentence -> 0}, []}]|\n",
      "|[{category, 0, 58, sadness, {surprise -> 9.781701E-8, joy -> 1.0658228E-8, fear -> 5.8513685E-8, sadness -> 0.9999999, sentence -> 0}, []}]  |\n",
      "|[{category, 0, 109, fear, {surprise -> 1.6646996E-9, joy -> 2.359295E-10, fear -> 1.0, sadness -> 4.3215704E-12, sentence -> 0}, []}]        |\n",
      "|[{category, 0, 43, joy, {surprise -> 0.005493851, joy -> 0.95643336, fear -> 0.0028001422, sadness -> 0.035272676, sentence -> 0}, []}]      |\n",
      "|[{category, 0, 1175, fear, {surprise -> 1.0530039E-8, joy -> 6.5194463E-9, fear -> 1.0, sadness -> 8.951719E-10, sentence -> 0}, []}]        |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\"sentiment_emotion\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda6467f-cce1-4136-9295-aba7e65383ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result1 = result.select(\n",
    "    F.expr(\"sentiment_emotion[0].result\").alias(\"emotion\"),\n",
    "    col(\"positive_score\").alias(\"sentiment_score\"),\n",
    "    col(\"subreddit\"),\n",
    "    col(\"text\"),\n",
    "    col(\"score\"),\n",
    "    col(\"num_comments\"),\n",
    "    col(\"over_18\"),\n",
    "    col(\"is_self\"),\n",
    "    col(\"is_video\"),\n",
    "    col(\"post_length\"),\n",
    "    col(\"hour_of_day\"),\n",
    "    col(\"day_of_week\"),\n",
    "    col(\"day_of_month\"),\n",
    "    col(\"month\"),\n",
    "    col(\"year\"),\n",
    "    col(\"has_media\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed4dac6e-4f79-40c7-81f2-d9ad84db42cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+\n",
      "| emotion|sentiment_score| subreddit|                text|score|num_comments|over_18|is_self|is_video|post_length|hour_of_day|day_of_week|day_of_month|month|year|has_media|\n",
      "+--------+---------------+----------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+\n",
      "|surprise|            1.0|    movies|sidney poitier a ...|17101|         501|  false|  false|   false|         53|         16|          7|          20|    2|2021|    false|\n",
      "| sadness|      0.5907684|     anime|help i get into a...|    0|          13|  false|   true|   false|         63|         16|          7|          20|    2|2021|    false|\n",
      "|    fear|      0.9160415|television|wwe elimination c...|    0|           1|  false|  false|   false|        120|         16|          7|          20|    2|2021|    false|\n",
      "|     joy|      0.6181543|    movies|chadwick bossman ...|    2|           0|  false|  false|   false|         48|         16|          7|          20|    2|2021|     true|\n",
      "|    fear|     0.20858091|    movies|crank and be the ...|  829|         197|  false|   true|   false|       1280|         16|          7|          20|    2|2021|    false|\n",
      "+--------+---------------+----------+--------------------+-----+------------+-------+-------+--------+-----------+-----------+-----------+------------+-----+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f11b321-177b-45ad-9625-cd9046f2da9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result1.write.parquet(\"s3a://project-group34/project/submissions/sentiment_emotion_submissions/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68253ec0-30c5-49d8-a25c-fa8f9e16742b",
   "metadata": {},
   "source": [
    "# ML MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd5614-6a92-4935-8550-8bc6248c4fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
