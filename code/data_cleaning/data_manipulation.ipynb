{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation for Author Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1699834573140
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-13T00:16:13.0433059Z",
       "execution_start_time": "2023-11-13T00:16:04.8164794Z",
       "livy_statement_state": "available",
       "parent_msg_id": "760e7fca-5e4c-4f1e-a63a-7e8091d5c328",
       "queued_time": "2023-11-13T00:15:12.9484087Z",
       "session_id": "107",
       "session_start_time": "2023-11-13T00:15:13.0015671Z",
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-13T00:16:10.526GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 6:\ndatastore = 'azureml://datastores/workspaceblobstore/paths/'\nsubmissions_path = 'filtered-submissions'\nsubmissions_df = spark.read.parquet(f\"{datastore}{submissions_path}\")\n\n# take a subset of columns\ndf = submissions_df.select(\"subreddit\", \"author\", \"title\", \"selftext\",\n                             \"created_utc\", \"num_comments\", \"score\", \n                             \"over_18\", \"media\", \"pinned\", \"locked\", \n                             \"disable_comments\", \"domain\", \"hidden\", \n                             \"distinguished\", \"hide_score\")\n\nfrom pyspark.sql.functions import length\n\n# Assuming your DataFrame is named `df`\ndf = df.withColumn('post_length', length(df.title) + length(df.selftext))\n\nfrom pyspark.sql import functions as F\n\ndf = df.withColumn('created_utc', F.to_timestamp('created_utc'))\n\n# Extract time-based features\ndf = df.withColumn('hour_of_day', F.hour('created_utc'))\ndf = df.withColumn('day_of_week', F.dayofweek('created_utc'))  # 1 (Sunday) to 7 (Saturday)\n# Map each day of the week from numeric...",
          "displayName": "parquet at NativeMethodAccessorImpl.java:0",
          "jobGroup": "6",
          "jobId": 0,
          "killedTasksSummary": {},
          "name": "parquet at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 0,
          "stageIds": [
           0
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-13T00:16:07.022GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 1,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 6
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 6, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datastore = 'azureml://datastores/workspaceblobstore/paths/'\n",
    "submissions_path = 'filtered-submissions'\n",
    "submissions_df = spark.read.parquet(f\"{datastore}{submissions_path}\")\n",
    "\n",
    "# take a subset of columns\n",
    "df = submissions_df.select(\"subreddit\", \"author\", \"title\", \"selftext\",\n",
    "                             \"created_utc\", \"num_comments\", \"score\", \n",
    "                             \"over_18\", \"media\", \"pinned\", \"locked\", \n",
    "                             \"disable_comments\", \"domain\", \"hidden\", \n",
    "                             \"distinguished\", \"hide_score\")\n",
    "\n",
    "# Calculate post length\n",
    "df = df.withColumn('post_length', length(df.title) + length(df.selftext))\n",
    "\n",
    "df = df.withColumn('created_utc', F.to_timestamp('created_utc'))\n",
    "\n",
    "# Extract time-based features\n",
    "df = df.withColumn('hour_of_day', F.hour('created_utc'))\n",
    "df = df.withColumn('day_of_week', F.dayofweek('created_utc'))  # 1 (Sunday) to 7 (Saturday)\n",
    "\n",
    "# Map each day of the week from numeric to string\n",
    "df = df.withColumn('day_of_week_str', F.expr(\"\"\"\n",
    "    CASE day_of_week \n",
    "        WHEN 1 THEN 'Sunday'\n",
    "        WHEN 2 THEN 'Monday'\n",
    "        WHEN 3 THEN 'Tuesday'\n",
    "        WHEN 4 THEN 'Wednesday'\n",
    "        WHEN 5 THEN 'Thursday'\n",
    "        WHEN 6 THEN 'Friday'\n",
    "        WHEN 7 THEN 'Saturday'\n",
    "    END\n",
    "\"\"\"))\n",
    "df = df.withColumn('day_of_month', F.dayofmonth('created_utc'))\n",
    "df = df.withColumn('month', F.month('created_utc'))\n",
    "df = df.withColumn('year', F.year('created_utc'))\n",
    "\n",
    "df = df.withColumn('has_media', F.col('media').isNotNull())\n",
    "\n",
    "df = df.drop(*[\"media\", \"disable_comments\", \"distinguished\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699833292558
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T23:54:52.489711Z",
       "execution_start_time": "2023-11-12T23:54:52.190417Z",
       "livy_statement_state": "available",
       "parent_msg_id": "4769cd0c-b9be-488c-aa0e-a7292d71fd02",
       "queued_time": "2023-11-12T23:54:52.0633237Z",
       "session_id": "106",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 19
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 106, 19, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PLOT_DIR = os.path.join(\"Users/st1140/fall-2023-reddit-project-team-34/data\", \"plots\")\n",
    "CSV_DIR = os.path.join(\"Users/st1140/fall-2023-reddit-project-team-34/data\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699760250851
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:37:30.6646577Z",
       "execution_start_time": "2023-11-12T03:37:30.3559882Z",
       "livy_statement_state": "available",
       "parent_msg_id": "23d2cad2-a79b-4705-8d8b-a40dafe45c3e",
       "queued_time": "2023-11-12T03:37:30.2175334Z",
       "session_id": "97",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 15
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 97, 15, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- pinned: boolean (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- hidden: boolean (nullable = true)\n",
      " |-- hide_score: boolean (nullable = true)\n",
      " |-- post_length: integer (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_of_week_str: string (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- has_media: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Table for Top comments of all sub reddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699761018132
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:50:18.0429887Z",
       "execution_start_time": "2023-11-12T03:50:17.2498361Z",
       "livy_statement_state": "available",
       "parent_msg_id": "2efc5695-5d14-47e2-849b-d02ed8fc57e9",
       "queued_time": "2023-11-12T03:50:17.0947208Z",
       "session_id": "98",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 8
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 98, 8, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
    "filtered_df = df.filter(df.subreddit.isin('movies', 'anime', 'television'))\n",
    "\n",
    "# Select columns for Plotly DataFrame\n",
    "df_plotly = filtered_df.select([\"subreddit\", \"title\",\"num_comments\", \"selftext\",\"author\",\"score\"])\n",
    "\n",
    "# Order the DataFrame in descending order based on the num_comments column\n",
    "df_plotly_sorted = df_plotly.orderBy(desc(\"num_comments\"))\n",
    "\n",
    "# Retrieve the top 20 rows\n",
    "df_top_20 = df_plotly_sorted.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699760341038
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:39:00.8248333Z",
       "execution_start_time": "2023-11-12T03:37:51.6841891Z",
       "livy_statement_state": "available",
       "parent_msg_id": "f8909676-56c0-4a72-affa-7dd9666872e8",
       "queued_time": "2023-11-12T03:37:51.480767Z",
       "session_id": "97",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-12T03:38:58.817GMT",
          "dataRead": 927859369,
          "dataWritten": 847531,
          "description": "Job group for statement 17:\ndf_top_20_pd=df_top_20.toPandas()",
          "displayName": "toPandas at /tmp/ipykernel_18002/2722203675.py:1",
          "jobGroup": "17",
          "jobId": 4,
          "killedTasksSummary": {},
          "name": "toPandas at /tmp/ipykernel_18002/2722203675.py:1",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 101,
          "numCompletedStages": 2,
          "numCompletedTasks": 101,
          "numFailedStages": 0,
          "numFailedTasks": 12,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 101,
          "rowCount": 1020968,
          "stageIds": [
           5,
           6
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-12T03:37:51.849GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 1,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 17
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 97, 17, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd=df_top_20.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699760347450
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:39:07.3568649Z",
       "execution_start_time": "2023-11-12T03:39:05.8505653Z",
       "livy_statement_state": "available",
       "parent_msg_id": "131bb0b2-fd55-4a6d-aab4-6090d60708ab",
       "queued_time": "2023-11-12T03:39:05.6798892Z",
       "session_id": "97",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 18
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 97, 18, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd.to_csv(f\"{CSV_DIR}/top_comments_eda_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Number of posts by Authors with Top Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699761104026
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:51:43.8901022Z",
       "execution_start_time": "2023-11-12T03:50:30.3557883Z",
       "livy_statement_state": "available",
       "parent_msg_id": "16252807-06be-4d25-b34d-95825f3d0490",
       "queued_time": "2023-11-12T03:50:30.2345537Z",
       "session_id": "98",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-12T03:51:43.408GMT",
          "dataRead": 23812,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n# Assuming your DataFrame is named 'df' and has a column named 'author'\nauthors_to_count = [\"dpemerson76\", \"lionsgate\", \"LiteraryBoner\",\"AnimeMod\",\"prsnreddit\",\"officialtobeymaguire\",\"KillerQ97\",\"leedavis1987\",\"Jeff_Souza\"]\n\n# Filter the DataFrame based on the specified authors\nfiltered_df = df.filter(col(\"author\").isin(authors_to_count))\n\n# Count the occurrences of each author\nauthor_counts = filtered_df.groupBy(\"author\").count()\n\n# Show the result\nauthor_counts.show()",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "9",
          "jobId": 2,
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 100,
          "numTasks": 101,
          "rowCount": 309,
          "stageIds": [
           2,
           3
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-12T03:51:42.906GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-11-12T03:51:42.636GMT",
          "dataRead": 765019597,
          "dataWritten": 23812,
          "description": "Job group for statement 9:\n# Assuming your DataFrame is named 'df' and has a column named 'author'\nauthors_to_count = [\"dpemerson76\", \"lionsgate\", \"LiteraryBoner\",\"AnimeMod\",\"prsnreddit\",\"officialtobeymaguire\",\"KillerQ97\",\"leedavis1987\",\"Jeff_Souza\"]\n\n# Filter the DataFrame based on the specified authors\nfiltered_df = df.filter(col(\"author\").isin(authors_to_count))\n\n# Count the occurrences of each author\nauthor_counts = filtered_df.groupBy(\"author\").count()\n\n# Show the result\nauthor_counts.show()",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "9",
          "jobId": 1,
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 100,
          "numCompletedStages": 1,
          "numCompletedTasks": 100,
          "numFailedStages": 0,
          "numFailedTasks": 15,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 100,
          "rowCount": 372727,
          "stageIds": [
           1
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-12T03:50:32.103GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 2,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 98, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              author|count|\n",
      "+--------------------+-----+\n",
      "|       LiteraryBoner|  549|\n",
      "|          prsnreddit|  519|\n",
      "|            AnimeMod|  755|\n",
      "|          Jeff_Souza|    4|\n",
      "|           lionsgate|    9|\n",
      "|        leedavis1987|    4|\n",
      "|officialtobeymaguire|    1|\n",
      "|         dpemerson76|    4|\n",
      "|           KillerQ97|    8|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is named 'df' and has a column named 'author'\n",
    "authors_to_count = [\"dpemerson76\", \"lionsgate\", \"LiteraryBoner\",\"AnimeMod\",\"prsnreddit\",\"officialtobeymaguire\",\"KillerQ97\",\"leedavis1987\",\"Jeff_Souza\"]\n",
    "\n",
    "# Filter the DataFrame based on the specified authors\n",
    "filtered_df = df.filter(col(\"author\").isin(authors_to_count))\n",
    "\n",
    "# Count the occurrences of each author\n",
    "author_counts = filtered_df.groupBy(\"author\").count()\n",
    "\n",
    "# Show the result\n",
    "author_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699761274185
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:54:34.0962067Z",
       "execution_start_time": "2023-11-12T03:54:33.7966409Z",
       "livy_statement_state": "available",
       "parent_msg_id": "a2f279b3-fe74-45e1-8f4c-512be5488aee",
       "queued_time": "2023-11-12T03:54:33.6627647Z",
       "session_id": "98",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 10
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 98, 10, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joinedDF = df_top_20.join(author_counts.select('author', 'count'), ['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Top 20 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699828827295
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:40:27.2152651Z",
       "execution_start_time": "2023-11-12T22:40:26.3867188Z",
       "livy_statement_state": "available",
       "parent_msg_id": "3524b996-647d-424c-8da1-6573a23e6919",
       "queued_time": "2023-11-12T22:40:26.2919811Z",
       "session_id": "101",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 8
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 101, 8, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
    "filtered_df_movies = df.filter(df.subreddit.isin('movies'))\n",
    "\n",
    "# Select columns for Plotly DataFrame\n",
    "df_plotly_movies = filtered_df_movies.select([\"subreddit\",\"title\", \"num_comments\", \"selftext\",\"author\",\"score\"])\n",
    "\n",
    "# Order the DataFrame in descending order based on the num_comments column\n",
    "df_plotly_sorted_movies = df_plotly_movies.orderBy(desc(\"num_comments\"))\n",
    "\n",
    "# Retrieve the top 20 rows\n",
    "df_top_20_movies = df_plotly_sorted_movies.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699828924379
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:42:04.0515907Z",
       "execution_start_time": "2023-11-12T22:40:30.7040414Z",
       "livy_statement_state": "available",
       "parent_msg_id": "8e0a868c-3f63-422e-baa2-794cc8d88b91",
       "queued_time": "2023-11-12T22:40:30.5804917Z",
       "session_id": "101",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-12T22:42:01.478GMT",
          "dataRead": 953349237,
          "dataWritten": 805557,
          "description": "Job group for statement 9:\ndf_top_20_pd_movies=df_top_20_movies.toPandas()",
          "displayName": "toPandas at /tmp/ipykernel_7280/739558151.py:1",
          "jobGroup": "9",
          "jobId": 1,
          "killedTasksSummary": {},
          "name": "toPandas at /tmp/ipykernel_7280/739558151.py:1",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 101,
          "numCompletedStages": 2,
          "numCompletedTasks": 101,
          "numFailedStages": 0,
          "numFailedTasks": 12,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 101,
          "rowCount": 1020968,
          "stageIds": [
           1,
           2
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-12T22:40:38.034GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 1,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 101, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd_movies=df_top_20_movies.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829062556
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:44:22.4095197Z",
       "execution_start_time": "2023-11-12T22:44:20.8020885Z",
       "livy_statement_state": "available",
       "parent_msg_id": "a8a7bf13-42ab-4828-9d2d-4f10de28979d",
       "queued_time": "2023-11-12T22:44:20.6354055Z",
       "session_id": "101",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 10
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 101, 10, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd_movies.to_csv(f\"{CSV_DIR}/top_comments_movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Top 20 anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829371083
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:49:31.0176013Z",
       "execution_start_time": "2023-11-12T22:49:28.8813191Z",
       "livy_statement_state": "available",
       "parent_msg_id": "81e907d0-928e-4a8e-80aa-993d8df0e141",
       "queued_time": "2023-11-12T22:49:28.7398623Z",
       "session_id": "102",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 8
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 102, 8, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
    "filtered_df_anime = df.filter(df.subreddit.isin('anime'))\n",
    "\n",
    "# Select columns for Plotly DataFrame\n",
    "df_plotly_anime = filtered_df_anime.select([\"subreddit\",\"title\", \"num_comments\", \"selftext\",\"author\",\"score\"])\n",
    "\n",
    "# Order the DataFrame in descending order based on the num_comments column\n",
    "df_plotly_sorted_anime = df_plotly_anime.orderBy(desc(\"num_comments\"))\n",
    "\n",
    "# Retrieve the top 20 rows\n",
    "df_top_20_anime = df_plotly_sorted_anime.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829454940
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:50:54.8763664Z",
       "execution_start_time": "2023-11-12T22:49:33.1009222Z",
       "livy_statement_state": "available",
       "parent_msg_id": "eba8d0b0-9085-4df2-97eb-15863adcf831",
       "queued_time": "2023-11-12T22:49:32.9128286Z",
       "session_id": "102",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-12T22:50:53.520GMT",
          "dataRead": 953494468,
          "dataWritten": 950788,
          "description": "Job group for statement 9:\ndf_top_20_pd_anime = df_top_20_anime.toPandas()",
          "displayName": "toPandas at /tmp/ipykernel_21080/3063620563.py:1",
          "jobGroup": "9",
          "jobId": 1,
          "killedTasksSummary": {},
          "name": "toPandas at /tmp/ipykernel_21080/3063620563.py:1",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 101,
          "numCompletedStages": 2,
          "numCompletedTasks": 101,
          "numFailedStages": 0,
          "numFailedTasks": 12,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 101,
          "rowCount": 1020968,
          "stageIds": [
           1,
           2
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-12T22:49:38.781GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 1,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 102, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd_anime = df_top_20_anime.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829486826
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:51:26.6999805Z",
       "execution_start_time": "2023-11-12T22:51:24.443555Z",
       "livy_statement_state": "available",
       "parent_msg_id": "20ebaf89-4c84-451a-a7e5-01a35bebbf8a",
       "queued_time": "2023-11-12T22:51:24.2954386Z",
       "session_id": "102",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 10
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 102, 10, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd_anime.to_csv(f\"{CSV_DIR}/top_comments_anime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Top 20 TV shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829649445
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:54:09.37352Z",
       "execution_start_time": "2023-11-12T22:54:08.5156792Z",
       "livy_statement_state": "available",
       "parent_msg_id": "4eb3fef1-87b8-4941-8e5d-5a67e9eb70f5",
       "queued_time": "2023-11-12T22:54:08.3493722Z",
       "session_id": "104",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 8
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 104, 8, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter the DataFrame for the subreddit 'anime', 'movies', and 'television'\n",
    "filtered_df_television = df.filter(df.subreddit.isin('television'))\n",
    "\n",
    "# Select columns for Plotly DataFrame\n",
    "df_plotly_television = filtered_df_television.select([\"subreddit\", \"num_comments\",\"title\", \"selftext\",\"author\",\"score\"])\n",
    "\n",
    "# Order the DataFrame in descending order based on the num_comments column\n",
    "df_plotly_sorted_television = df_plotly_television.orderBy(desc(\"num_comments\"))\n",
    "\n",
    "# Retrieve the top 20 rows\n",
    "df_top_20_television = df_plotly_sorted_television.limit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829725951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:55:25.8779982Z",
       "execution_start_time": "2023-11-12T22:54:10.6433001Z",
       "livy_statement_state": "available",
       "parent_msg_id": "0f46e660-db14-4f55-b0ba-9112cd816c27",
       "queued_time": "2023-11-12T22:54:10.4445065Z",
       "session_id": "104",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-12T22:55:24.181GMT",
          "dataRead": 953587196,
          "dataWritten": 1043516,
          "description": "Job group for statement 9:\ndf_top_20_pd_television=df_top_20_television.toPandas()",
          "displayName": "toPandas at /tmp/ipykernel_23882/1382704528.py:1",
          "jobGroup": "9",
          "jobId": 1,
          "killedTasksSummary": {},
          "name": "toPandas at /tmp/ipykernel_23882/1382704528.py:1",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 101,
          "numCompletedStages": 2,
          "numCompletedTasks": 101,
          "numFailedStages": 0,
          "numFailedTasks": 14,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 101,
          "rowCount": 1026968,
          "stageIds": [
           1,
           2
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-12T22:54:12.486GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 1,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 104, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd_television=df_top_20_television.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699829778161
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T22:56:18.11008Z",
       "execution_start_time": "2023-11-12T22:56:16.5997269Z",
       "livy_statement_state": "available",
       "parent_msg_id": "e404d364-cf67-4a60-8755-a6711a043ed3",
       "queued_time": "2023-11-12T22:56:16.4977417Z",
       "session_id": "104",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 10
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 104, 10, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_top_20_pd_television.to_csv(f\"{CSV_DIR}/top_comments_tv_show.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699758438342
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-12T03:07:18.2632977Z",
       "execution_start_time": "2023-11-12T03:07:17.9706556Z",
       "livy_statement_state": "available",
       "parent_msg_id": "281cb801-6dd6-419b-bffe-8252d3c27268",
       "queued_time": "2023-11-12T03:07:17.8244281Z",
       "session_id": "95",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 10
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 95, 10, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group by 'author' and count the occurrences\n",
    "author_counts = df.groupBy(\"author\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699834632322
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-13T00:17:12.2119827Z",
       "execution_start_time": "2023-11-13T00:17:10.7322195Z",
       "livy_statement_state": "available",
       "parent_msg_id": "80a28e7f-8448-4ba7-b139-7548946b8b05",
       "queued_time": "2023-11-13T00:17:10.6135506Z",
       "session_id": "107",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 7
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 7, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read all the csvs\n",
    "df_movies = pd.read_csv(\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/top_comments_movies.csv\")\n",
    "df_anime = pd.read_csv(\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/top_comments_anime.csv\")\n",
    "df_tv = pd.read_csv(\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/top_comments_tv_show.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699834715529
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-13T00:18:35.4021809Z",
       "execution_start_time": "2023-11-13T00:17:14.035103Z",
       "livy_statement_state": "available",
       "parent_msg_id": "4162d4d0-ffd3-4810-a9ae-2b3aa5c5b5ae",
       "queued_time": "2023-11-13T00:17:13.8490794Z",
       "session_id": "107",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-11-13T00:18:32.532GMT",
          "dataRead": 147629,
          "dataWritten": 0,
          "description": "Job group for statement 8:\nresult = pd.concat([df_movies, df_anime, df_tv], axis=0)\nunique_authors = list(result[[\"subreddit\", \"author\"]].drop_duplicates()[\"author\"].unique())\ncount_of_posts_per_author = df.filter(df.author.isin(unique_authors)).groupBy(\"author\").count().toPandas()",
          "displayName": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
          "jobGroup": "8",
          "jobId": 2,
          "killedTasksSummary": {},
          "name": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 100,
          "numTasks": 101,
          "rowCount": 1897,
          "stageIds": [
           2,
           3
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-13T00:18:31.647GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2023-11-13T00:18:31.460GMT",
          "dataRead": 1134165430,
          "dataWritten": 147629,
          "description": "Job group for statement 8:\nresult = pd.concat([df_movies, df_anime, df_tv], axis=0)\nunique_authors = list(result[[\"subreddit\", \"author\"]].drop_duplicates()[\"author\"].unique())\ncount_of_posts_per_author = df.filter(df.author.isin(unique_authors)).groupBy(\"author\").count().toPandas()",
          "displayName": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
          "jobGroup": "8",
          "jobId": 1,
          "killedTasksSummary": {},
          "name": "toPandas at /tmp/ipykernel_21564/1576348934.py:3",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 100,
          "numCompletedStages": 1,
          "numCompletedTasks": 100,
          "numFailedStages": 0,
          "numFailedTasks": 15,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 100,
          "rowCount": 1018865,
          "stageIds": [
           1
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-11-13T00:17:16.308GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 2,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 8
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 8, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculating count of posts per author\n",
    "result = pd.concat([df_movies, df_anime, df_tv], axis=0)\n",
    "unique_authors = list(result[[\"subreddit\", \"author\"]].drop_duplicates()[\"author\"].unique())\n",
    "count_of_posts_per_author = df.filter(df.author.isin(unique_authors)).groupBy(\"author\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1699834725167
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-11-13T00:18:45.0762001Z",
       "execution_start_time": "2023-11-13T00:18:44.7785651Z",
       "livy_statement_state": "available",
       "parent_msg_id": "4de98614-68a0-4f08-a0d5-590dece503a1",
       "queued_time": "2023-11-13T00:18:44.6309336Z",
       "session_id": "107",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "9d2667d4-d95f-4c64-adf7-bfab734cf5c0",
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(9d2667d4-d95f-4c64-adf7-bfab734cf5c0, 107, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# merging and storing to csv\n",
    "authors_with_top_comments_post_counts = result.merge(count_of_posts_per_author, how=\"left\", on=\"author\")\n",
    "authors_with_top_comments_post_counts.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "authors_with_top_comments_post_counts = authors_with_top_comments_post_counts[authors_with_top_comments_post_counts['author'] != '[deleted]']\n",
    "authors_with_top_comments_post_counts.to_csv(f\"Users/st1140/fall-2023-reddit-project-team-34/data/csv/authors_with_top_comments_post_counts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
