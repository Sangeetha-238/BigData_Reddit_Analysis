{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Spark Job on AzureML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a client connection to the AzureML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1699633450251
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient, spark, Input, Output\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import UserIdentityConfiguration\n",
    "\n",
    "azureml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Job\n",
    "\n",
    "The following cell defines the job. It is an object of [Spark Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.spark?view=azure-python) that contains the required information to run a job:\n",
    "\n",
    "- The cluster size\n",
    "- The script to run\n",
    "- The parameters for the script\n",
    "\n",
    "In the example below, we are using the `pyspark-script-job.py` script which is parametrized. As you can see, the parameters are the following:\n",
    "\n",
    "- `platform`: which can be `azureml` or `sagemaker`. This script is designed to be able to run on both platforms.\n",
    "- `input_object_store_base_url`: Here you will use a base URL of the `s3://<BUCKETNAME>/` form for Sagemaker, or `wasbs://<CONTAINER-NAME>@<STORAGE-ACCOUNT>.blob.core.windows.net/` or `azureml://datastores/workspaceblobstore/paths/` for AzureML. **Don't forget the trailing slash /.**\n",
    "- `input_path`: The path to read from\n",
    "- `output_object_store_base_url`: Here you will use a base URL of the `s3://<BUCKETNAME>/` form for Sagemaker, or `wasbs://<CONTAINER-NAME>@<STORAGE-ACCOUNT>.blob.core.windows.net/` or `azureml://datastores/workspaceblobstore/paths/` for AzureML. **Don't forget the trailing slash /.**\n",
    "- `output_path`: The path to write to\n",
    "- `subreddits`: a comma separated string of subreddit names\n",
    "\n",
    "This job receives the object store location for the raw data, in this case a single month. Then the job filters the original data and writes the filtered data out. This is designed to be used for either submissions or comments, not both.\n",
    "\n",
    "For more information about the parameters used in the job definition, [read the documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-submit-spark-jobs?view=azureml-api-2&tabs=sdk#submit-a-standalone-spark-job).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1699633481090
    }
   },
   "outputs": [],
   "source": [
    "spark_job = spark(\n",
    "    name=\"reddit-filter-submissions\",\n",
    "    code=\"./\",\n",
    "    entry={\"file\": \"pyspark-script-job.py\"},\n",
    "    driver_cores=1,\n",
    "    driver_memory=\"5g\",\n",
    "    executor_cores=4,\n",
    "    executor_memory=\"10g\",\n",
    "    executor_instances=6,\n",
    "    resources={\n",
    "        \"instance_type\": \"Standard_E4S_V3\",\n",
    "        \"runtime_version\": \"3.2.0\",\n",
    "    },\n",
    "    inputs={\n",
    "        \"platform\": \"azureml\",\n",
    "        \"input_object_store_base_url\": \"wasbs://bigdata@marckvnonprodblob.blob.core.windows.net/\",\n",
    "        \"input_path\": \"reddit-parquet/submissions/year=*/month=*/\",\n",
    "        \"output_object_store_base_url\": \"azureml://datastores/workspaceblobstore/paths/\",\n",
    "        \"output_path\": \"filtered-submissions\",\n",
    "        \"subreddits\": \"movies,television,anime,MovieSuggestions,televisionsuggestions,Animesuggest\"\n",
    "    },\n",
    "    identity=UserIdentityConfiguration(),\n",
    "    args=\"--platform ${{inputs.platform}} --input_object_store_base_url ${{inputs.input_object_store_base_url}} --input_path ${{inputs.input_path}} --output_object_store_base_url ${{inputs.output_object_store_base_url}} --output_path ${{inputs.output_path}} --subreddits ${{inputs.subreddits}}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script.py File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pyspark-script-job.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Parse Inputs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--platform\")\n",
    "parser.add_argument(\"--input_object_store_base_url\")\n",
    "parser.add_argument(\"--input_path\")\n",
    "parser.add_argument(\"--output_object_store_base_url\")\n",
    "parser.add_argument(\"--output_path\")\n",
    "parser.add_argument(\"--subreddits\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.info(args.platform)\n",
    "logging.info(args.input_object_store_base_url)\n",
    "logging.info(args.input_path)\n",
    "logging.info(args.output_object_store_base_url)\n",
    "logging.info(args.output_path)\n",
    "logging.info(args.subreddits)\n",
    "\n",
    "input_complete_path = f\"{args.input_object_store_base_url}{args.input_path}\"\n",
    "output_complete_path = f\"{args.output_object_store_base_url}{args.output_path}\"\n",
    "\n",
    "logging.info(input_complete_path)\n",
    "logging.info(output_complete_path)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "logging.info(f\"spark version = {spark.version}\")\n",
    "\n",
    "if args.platform == \"azureml\":\n",
    "    # Add the Blob SAS credentials\n",
    "    blob_account_name = \"marckvnonprodblob\"\n",
    "    blob_container_name = \"bigdata\"\n",
    "    blob_sas_token = \"sv=2021-10-04&st=2023-10-04T20%3A02%3A01Z&se=2023-12-31T21%3A02%3A00Z&sr=c&sp=racwdxltf&sig=l%2BbUjYGp1p2IDeyanWtXpDjssBCdW%2B4CJlO4SfPnCEk%3D\"\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\n",
    "        blob_sas_token,\n",
    "    )\n",
    "\n",
    "\n",
    "# Read data from object store\n",
    "logging.info(f\"going to read {input_complete_path}\")\n",
    "df_in = spark.read.parquet(input_complete_path)\n",
    "df_in_ct = df_in.count()\n",
    "logging.info(f\"finished reading files...\")\n",
    "\n",
    "# filter the dataframe to only keep the subreddits of interest\n",
    "subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "filtered = df_in.where(col(\"subreddit\").isin(subreddits))\n",
    "filtered_ct = filtered.count()\n",
    "\n",
    "# save the filtered dataframes so that these files can now be used for future analysis\n",
    "logging.info(f\"going to write {output_complete_path}\")\n",
    "\n",
    "logging.info(f\"Read in {df_in_ct} records, wrote out {filtered_ct} records.\")\n",
    "filtered.write.mode(\"overwrite\").parquet(output_complete_path, compression=\"zstd\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1699633485931
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading code (0.52 MBs): 100%|██████████| 519916/519916 [00:00<00:00, 3784592.11it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_object = azureml_client.jobs.create_or_update(spark_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Job Studio URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1699633485993
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ml.azure.com/runs/reddit-filter-submissions?wsid=/subscriptions/eccec620-0403-4f08-9535-0f93c5a0e1dc/resourcegroups/project-rg/workspaces/group-34-aml&tid=fd571193-38cb-437b-bb55-60f28d67b643\n"
     ]
    }
   ],
   "source": [
    "studio_url = job_object.studio_url\n",
    "print(studio_url)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aeff17a1aa7735c2f7cb3a6d691fe1b4d4c3b8d2d650f644ad0f24e1b8e3f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
