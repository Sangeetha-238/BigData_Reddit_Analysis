<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.246">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./img/home_icon.png" alt="" class="navbar-logo">
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html">
 <span class="menu-text">Introduction</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./eda.html">
 <span class="menu-text">EDA</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-nlp" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">NLP</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-nlp">    
        <li>
    <a class="dropdown-item" href="./nlp.html">
 <span class="dropdown-text">Analysis Report</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./nlp_exec_summary.html">
 <span class="dropdown-text">Executive Summary</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-ml" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">ML</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-ml">    
        <li>
    <a class="dropdown-item" href="./ml.html">
 <span class="dropdown-text">Analysis Report</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ml_exec_summary.html">
 <span class="dropdown-text">Executive Summary</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./conclusion.html">
 <span class="menu-text">Conclusion</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="button" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/gu-dsan6000/fall-2023-reddit-project-team-34">
 <span class="dropdown-text">Source Code</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Machine Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/hero_ml.gif" class="img-fluid figure-img" style="width:30.0%"></p>
</figure>
</div>
<section id="anime-and-movies-machine-learning-classification-of-subreddits" class="level2">
<h2 class="anchored" data-anchor-id="anime-and-movies-machine-learning-classification-of-subreddits">Anime and Movies: Machine Learning Classification of Subreddits</h2>
<p>In the vast and ever-expanding universe of Reddit, where countless subreddits coexist, the ability to classify posts accurately is crucial for both content curation and user experience. By distinguishing between ‘Anime’ and ‘Movies’ - two distinct categories with passionate followings - this classification project aims to streamline content discovery, enhance recommendation algorithms, and foster community engagement. As users navigate through the platform, accurately categorized posts ensure that they find the content that resonates with their interests swiftly. For content moderators and advertisers, such classification provides a clear understanding of discussion trends and audience preferences, enabling more targeted and effective engagement strategies. Ultimately, this machine learning endeavor seeks to contribute to a more personalized, organized, and engaging Reddit experience for every user.</p>
<section id="data-cleaning" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning">Data Cleaning</h3>
<p><strong>Feature Engineering</strong></p>
<p>The initial step in the data preprocessing pipeline involved enhancing the dataset with new features that could be crucial for the classification of Reddit posts.</p>
<p><code>Post Length Calculation</code> The total length of each post was calculated by summing the lengths of the post’s title and text.</p>
<p><code>Time-Based Feature Extraction</code> Time-related features were extracted from the ‘created_utc’ timestamp. These included the hour of the day, day of the week, day of the month, month, and year.</p>
<p><code>Media Presence Identification</code> A boolean feature ‘has_media’ was created to indicate whether a post contains media.</p>
<p><code>Feature Selection</code> The dataset was refined to include only relevant features for the classification task, ensuring a focused and efficient modeling process.</p>
<p><strong>Text Preprocessing</strong></p>
<p>Textual data from Reddit posts underwent several preprocessing steps to standardize and prepare it for machine learning analysis.</p>
<p><code>Combining Text Fields</code> The ‘title’ and ‘selftext’ fields were combined into a single ‘body’ text field, consolidating all textual information into one comprehensive feature.</p>
<p><code>Handling Missing Values</code> Rows with missing ‘domain’ values were removed to maintain data integrity and consistency.</p>
<p><code>Text Normalization</code> The ‘body’ text was converted to lowercase, and newline characters and punctuations were removed. This normalization is crucial for reducing the complexity of text data, facilitating more effective machine learning analysis.</p>
<p><strong>Text Vectorization</strong></p>
<p>The processed text data was transformed into a numerical format suitable for machine learning models.</p>
<p><code>Tokenization and Stop Words Removal</code> The body text was tokenized into individual words, and common stop words were removed to emphasize more meaningful content.</p>
<p><code>Feature Vector Creation</code> The HashingTF method was utilized to convert the processed text into numerical feature vectors. The IDF (Inverse Document Frequency) was applied to rescale these vectors, highlighting significant terms for differentiating between ‘Anime’ and ‘Movies’.</p>
<p><strong>Final Data Preparation</strong></p>
<p>The concluding steps in data preparation involved schema adjustments and data type conversions, optimizing the dataset for the modeling stage.</p>
<p><code>Schema Adjustment and Cleanup</code> Unnecessary columns were removed from the dataset, and the final schema was verified to include only pertinent features.</p>
<p><code>Data Type Conversion</code> Boolean columns were cast to strings to align with the requirements of the machine learning algorithms in the subsequent stages.</p>
</section>
<section id="data-pre-processing-for-modeling" class="level3">
<h3 class="anchored" data-anchor-id="data-pre-processing-for-modeling">Data Pre-Processing for Modeling</h3>
<p><code>String Indexing</code> Key categorical features such as ‘over_18’, ‘is_self’, ‘is_video’, ‘has_media’, and ‘subreddit’ were transformed using StringIndexer. This step converted categorical strings into numerical indices, making them suitable for machine learning models.</p>
<p><code>One-Hot Encoding</code> The indexed categories were further encoded using OneHotEncoder. This process transformed the indexed categories into a binary vector representation, a necessary step for handling categorical data in many machine learning algorithms.</p>
<p><code>Vector Assembler</code> A VectorAssembler was utilized to combine various feature columns into a single vector column. This included both the newly created feature vectors from one-hot encoding and the existing numerical features such as ‘score’, ‘num_comments’, ‘post_length’, ‘hour_of_day’, ‘day_of_week’, ‘day_of_month’, ‘month’, and ‘year’. This assembly created a unified feature vector essential for feeding into the machine learning models.</p>
<p><code>Pipeline Definition</code> A Pipeline was defined incorporating all the stages of string indexing, one-hot encoding, and vector assembly. This approach streamlined the preprocessing steps and ensured consistency across the data.</p>
<p><code>Data Split train and test</code> Following thorough data cleansing, the dataset was partitioned into training and testing sets with an 80-20 split.</p>
<p><code>Pipeline Execution</code> The preprocessing pipeline was applied to the training and test data. This transformed the data according to the defined stages, ensuring that it was ready for model training and evaluation.</p>
</section>
<section id="model-training-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="model-training-and-evaluation">Model Training and Evaluation</h3>
<section id="model-selection" class="level4">
<h4 class="anchored" data-anchor-id="model-selection">Model Selection</h4>
<p>We selected a Random Forest classifier to classify between Anime and Movie-related Reddit posts primarily because random first gives highest accuracy among all classification models and its robustness, particularly in scenarios involving large datasets with numerous variables. It can also automatically balance the data.</p>
</section>
<section id="model-training" class="level4">
<h4 class="anchored" data-anchor-id="model-training">Model Training</h4>
<p>Initially, we opted for simplicity and started with a Random Forest model, using its default parameters. This approach was somewhat like dipping our toes in the water to gauge the temperature. After training, the results we got were quite encouraging - not exactly groundbreaking, but definitely on the right track. This outcome provided a solid foundation for us to build upon and refine our model further.</p>

<table style="width: 100%; border: none;">
  <tbody><tr>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/Heatmap1_CM_Classification_Model1.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-1 (a): Confusion Matrix for Default Random Forest Model</figcaption>
      </figure>
    </td>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/ROC1_Classification_Model1.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-1 (b): ROC curve for Default Random Forest Model</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td colspan="2" style="text-align: center; border: none;">
      <figcaption>Figure-1</figcaption>
    </td>
  </tr>
</tbody></table>

<!-- ![Figure-1: ](img/Heatmap1_CM_Classification_Model1.png){width="100%" fig-align="center"}
![Figure-2: ](img/ROC1_Classification_Model1.png){width="100%" fig-align="center"} -->
<p>The first try at using a Random Forest classifier to sort Reddit posts into ‘Anime’ or ‘Movies’ was a good start. The model worked alright, as shown by a score of 0.76 out of 1 on a test called the ROC curve, which measures how well the model distinguishes between the two categories. However, the model wasn’t perfect and often mixed up posts, especially mistaking ‘Movies’ posts for ‘Anime’. This tells us that while the model did an okay job, there’s still quite a bit of room to make it better by changing its settings.</p>
<p>The first results show that using the basic settings for the model isn’t the best for this job of telling Anime and Movie posts apart. The model could do a much better job if we adjust its settings, like how many decision trees to use, how deep these trees should go, and how many posts it should look at before making a decision. Changing these could make the model much better at correctly identifying posts.</p>
<p>To elevate our initial model, we delved into hyperparameter tuning. We experimented with a variety of settings for ‘numTrees’, ‘maxDepth’, and ‘maxBins’, testing different combinations. Ultimately,landed on the configuration that significantly enhanced our model’s accuracy and efficiency: numTrees=100, maxDepth=10, and maxBins=64. The model showed significantly better performance after hyperparameter tuning.</p>

<table style="width: 100%; border: none;">
  <tbody><tr>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/Heatmap2_CM_Classification_Model1.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-2 (a): Confusion Matrix for Random Forest Model with Hyper parameters</figcaption>
      </figure>
    </td>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/ROC2_Classification_Model1.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-2 (b): ROC curve for Random Forest Model with Hyper parameters</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td colspan="2" style="text-align: center; border: none;">
      <figcaption>Figure-2</figcaption>
    </td>
  </tr>
</tbody></table>

<!-- ![Figure-3: ](img/Heatmap2_CM_Classification_Model1.png){width="100%" fig-align="center"}
![Figure-4: ](img/ROC2_Classification_Model1.png){width="100%" fig-align="center"} -->
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy Train</th>
<th>Accuracy Test</th>
<th>F1 Score Train</th>
<th>F1 Score Test</th>
<th>Precision Train</th>
<th>Precision Test</th>
<th>Recall Train</th>
<th>Recall Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RandomForest Default</td>
<td>0.676</td>
<td>0.675</td>
<td>0.642</td>
<td>0.642</td>
<td>0.760</td>
<td>0.760</td>
<td>0.676</td>
<td>0.675</td>
</tr>
<tr class="even">
<td>RandomForest Hyperparameters</td>
<td>0.760</td>
<td>0.759</td>
<td>0.745</td>
<td>0.744</td>
<td>0.829</td>
<td>0.828</td>
<td>0.760</td>
<td>0.759</td>
</tr>
</tbody>
</table>
<p>The Hyperparameter tuning for the Random Forest classifier yielded a significant improvement in the model’s performance for classifying Reddit posts into ‘Anime’ or ‘Movies’. By adjusting the number of trees (‘numTrees’), the depth of each tree (‘maxDepth’), and the number of bins (‘maxBins’), the model’s ability to correctly identify the categories has been notably enhanced.</p>
<p>The confusion matrix post-tuning presents a dramatic decrease in misclassified instances, particularly in distinguishing ‘Movies’ from ‘Anime’. Furthermore, the ROC curve, with an area under the curve (AUC) of 0.95, indicates a superior model performance in differentiating between the two classes as compared to the pre-tuned state, which had an AUC of 0.76. This stark contrast in the AUC values before and after tuning underscores the impact of fine-tuning hyperparameters.</p>
</section>
</section>
</section>
<section id="classifying-popularity-machine-learning-for-reddit-engagement-analysis" class="level2">
<h2 class="anchored" data-anchor-id="classifying-popularity-machine-learning-for-reddit-engagement-analysis">Classifying Popularity: Machine Learning for Reddit Engagement Analysis</h2>
<p>In the dynamic and diverse ecosystem of Reddit, where every submission vies for attention, being able to predict the popularity of a post holds immense value. The second model in this project endeavors to classify Reddit submissions as ‘popular’ or ‘not popular,’ serving a multitude of purposes. For content creators and marketers, understanding the elements that contribute to a post’s popularity can inform their strategy to capture the community’s interest more effectively. For the average user, this classification can enhance the personalization of their feed, bringing popular, engaging content to the forefront. Additionally, platform moderators can utilize insights from popularity predictions to better manage the influx of content and maintain the quality of user experience. This binary classification model, powered by machine learning, not only aims to decipher the attributes that propel a post to popularity but also to apply this knowledge in a way that enriches the Reddit landscape for all participants.</p>
<section id="data-cleaning-1" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning-1">Data Cleaning</h3>
<p><strong>Feature Engineering</strong></p>
<p>Along with the existing features, the dataset was enhanced with new features that could be crucial for the classification of Reddit posts.</p>
<p>To enhance the accuracy of predicting a post’s popularity, we introduced ‘Sentiment Score’ and ‘Emotion’ as new features, recognizing that the tone and emotional context of a post greatly influence user engagement. The sentiment score captures the overall positivity or negativity of the content, while the emotion feature taps into the specific feelings a post might evoke, both of which are key indicators of a post’s potential to resonate with and captivate the Reddit community.</p>
<p><code>Sentiment Analysis</code> The ‘body’ text was analyzed using the Spark NLP sentiment analysis model (sentimentdl_use_twitter) to extract sentiment score.</p>
<p><code>Emotion Analysis</code> The ‘body’ text was analyzed using the Spark NLP emotion analysis model (classifierdl_use_emotion) to extract different emotions.</p>
<p>All the other steps were the same as the above model.</p>
</section>
<section id="data-pre-processing-for-modeling-1" class="level3">
<h3 class="anchored" data-anchor-id="data-pre-processing-for-modeling-1">Data Pre-Processing for Modeling</h3>
<p><code>Target Variable Generation</code> The ‘num_comments’ column was used to generate the target variable ‘is_popular’ by applying a threshold of 27 (mean number of comments). Posts with a score of 27 or above were classified as ‘popular’ while those with a score below 27 were classified as ‘not popular’.</p>
<p><code>Handling Class Imbalance</code> The dataset was balanced by downsampling the majority class (‘not popular’) to match the minority class (‘popular’).</p>

<table style="width: 100%; border: none;">
  <tbody><tr>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/Class_Imbalance_Model2.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-3 (a): Target variable Distribution</figcaption>
      </figure>
    </td>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/Class_Imbalance_Model2-downsampled.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-3 (b): Target variable Distribution after downsampling</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td colspan="2" style="text-align: center; border: none;">
      <figcaption>Figure-3</figcaption>
    </td>
  </tr>
</tbody></table>
<!-- ![Figure-1: ](img/Class_Imbalance_Model2.png){width="100%" fig-align="center"}
![Figure-1: ](img/Class_Imbalance_Model2.png){width="100%" fig-align="center"} -->
<p><code>String Indexing</code> Key categorical features such as ‘over_18’, ‘is_self’, ‘is_video’, ‘has_media’, and ‘subreddit’ were transformed using StringIndexer. This step converted categorical strings into numerical indices, making them suitable for machine learning models.</p>
<p><code>One-Hot Encoding</code> The indexed categories were further encoded using OneHotEncoder. This process transformed the indexed categories into a binary vector representation, a necessary step for handling categorical data in many machine learning algorithms.</p>
<p><code>Vector Assembler</code> A VectorAssembler was utilized to combine various feature columns into a single vector column. This included both the newly created feature vectors from one-hot encoding and the existing numerical features such as ‘sentiment_score’, ‘score’, ‘post_length’, ‘hour_of_day’, ‘day_of_week’, ‘day_of_month’, ‘month’, and ‘year’. This assembly created a unified feature vector essential for feeding into the machine learning models.</p>
<p><code>Pipeline Definition</code> A Pipeline was defined incorporating all the stages of string indexing, one-hot encoding, and vector assembly. This approach streamlined the preprocessing steps and ensured consistency across the data.</p>
<p><code>Pipeline Execution</code> The preprocessing pipeline was applied to the training and test data. This transformed the data according to the defined stages, ensuring that it was ready for model training and evaluation.</p>
</section>
<section id="model-training-and-evaluation-1" class="level3">
<h3 class="anchored" data-anchor-id="model-training-and-evaluation-1">Model Training and Evaluation</h3>
<section id="model-selection-1" class="level4">
<h4 class="anchored" data-anchor-id="model-selection-1">Model Selection</h4>
<p>For predicting whether Reddit posts will be popular or not, we started with a simple Logistic Regression model. This model was chosen for its ease of use and good performance with basic yes/no type questions, like predicting if a post will be popular. However, to deal with the large and complex data from Reddit, we later switched to using a Decision Tree classifier. This model is better for handling lots of different factors in the data and makes it easier to understand how it decides if a post will be popular.</p>
</section>
<section id="model-training-1" class="level4">
<h4 class="anchored" data-anchor-id="model-training-1">Model Training</h4>
<p>In the task of predicting the popularity of Reddit posts, we initially utilized a Logistic Regression model. This model, known for its straightforwardness and effectiveness in binary classification, served as a fundamental starting point. It yielded promising results, demonstrating high accuracy in training (93.41%) but a moderate accuracy in testing (75.11%). This discrepancy indicated that while the model performed well on familiar data, it was less effective with new, unseen data.</p>

<table style="width: 100%; border: none;">
  <tbody><tr>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/Heatmap1_CM_Classification_Model2.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-4 (a): Confusion Matrix for Logistic Regression Model</figcaption>
      </figure>
    </td>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/ROC1_Classification_Model2.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-4 (b): ROC curve for Logistic Regression Model</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td colspan="2" style="text-align: center; border: none;">
      <figcaption>Figure-4</figcaption>
    </td>
  </tr>
</tbody></table>

<!-- ![Figure-1: ](img/Heatmap1_CM_Classification_Model1.png){width="100%" fig-align="center"}
![Figure-2: ](img/ROC1_Classification_Model1.png){width="100%" fig-align="center"} -->
<p>The initial Decision Tree model provided insightful outcomes. It showed a moderate level of accuracy in predicting post popularity, as evidenced by the ROC curve and other performance metrics. However, there were still instances of incorrect predictions, signaling the need for further model optimization.</p>
<p>To address this and capture the complexities of Reddit’s data more effectively, we transitioned to a Decision Tree classifier. The Decision Tree model, adept at handling diverse datasets, showed a more balanced performance with training accuracy at 87.44% and testing accuracy at 87.20%. This indicated a better generalization capability of the model on new data compared to the Logistic Regression model.</p>

<table style="width: 100%; border: none;">
  <tbody><tr>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/Heatmap2_CM_Classification_Model2.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-5 (a): Confusion Matrix for Decision Tree Model</figcaption>
      </figure>
    </td>
    <td style="width: 50%; text-align: center; vertical-align: bottom; border: none;">
      <figure class="figure">
        <img src="img/ROC2_Classification_Model2.png" width="100%" class="figure-img">
        <figcaption class="figure-caption">Figure-5 (b): ROC curve for Decision Tree Model</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td colspan="2" style="text-align: center; border: none;">
      <figcaption>Figure-5</figcaption>
    </td>
  </tr>
</tbody></table>

<!-- 
![Figure-3: ](img/Heatmap2_CM_Classification_Model1.png){width="100%" fig-align="center"}
![Figure-4: ](img/ROC2_Classification_Model1.png){width="100%" fig-align="center"} -->
<table class="table">
<colgroup>
<col style="width: 14%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy Train</th>
<th>Accuracy Test</th>
<th>F1 Score Train</th>
<th>F1 Score Test</th>
<th>Precision Train</th>
<th>Precision Test</th>
<th>Recall Train</th>
<th>Recall Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logistic Regression</td>
<td>0.934</td>
<td>0.751</td>
<td>0.934</td>
<td>0.750</td>
<td>0.934</td>
<td>0.751</td>
<td>0.934</td>
<td>0.751</td>
</tr>
<tr class="even">
<td>Decision Tree</td>
<td>0.874</td>
<td>0.872</td>
<td>0.874</td>
<td>0.872</td>
<td>0.874</td>
<td>0.872</td>
<td>0.874</td>
<td>0.872</td>
</tr>
</tbody>
</table>
<p>The comparison of these two models highlights the importance of choosing the right algorithm for specific data characteristics. The Decision Tree’s improved ability to generalize, as evidenced by the more consistent accuracy between training and testing, underscores its suitability for the dynamic and varied content on Reddit. This analysis demonstrates the value of model selection and tuning in the field of popularity prediction on social media platforms.</p>
</section>
</section>
</section>
<section id="predicting-submission-scores-a-machine-learning-approach-to-forecasting-reddit-scores" class="level2">
<h2 class="anchored" data-anchor-id="predicting-submission-scores-a-machine-learning-approach-to-forecasting-reddit-scores">Predicting Submission Scores: A Machine Learning Approach to Forecasting Reddit Scores</h2>
<p>In the busy world of Reddit, a post’s score, which is the total of its upvotes minus its downvotes, is really important for understanding how much users like the content. We’re looking at different things like what the post is about, when it was posted, and how users interact with it, to predict the score of a Reddit post. Being able to guess a post’s score helps us figure out what makes a post appealing to users. This is great for people who create content and those who manage the site, as it helps them know what their audience likes. We’re using advanced technology to figure out the secrets behind a successful Reddit post, aiming to make the platform even more engaging for its users.</p>
<section id="data-cleaning-2" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning-2">Data Cleaning</h3>
<p>The cleaning process for this model was similar to the previous model. The only difference was that we did not Vectorize (use TF-IDF) for the text data primarily because the focus of this model is different compared to previous models. Text vectorization techniques like TF-IDF are powerful for understanding and analyzing the content of text data, especially in tasks like classification where the text’s topic or sentiment is crucial. In predicting submission scores, the emphasis is likely on quantifiable metrics such as user interaction data (e.g., number of comments, time since posted) and post characteristics (e.g., length of the post, presence of links or media), which have a more direct and measurable impact on a post’s popularity as indicated by its score.</p>
</section>
<section id="data-pre-processing-for-modeling-2" class="level3">
<h3 class="anchored" data-anchor-id="data-pre-processing-for-modeling-2">Data Pre-Processing for Modeling</h3>
<p><code>String Indexing</code> Key categorical features such as ‘over_18’, ‘is_self’, ‘is_video’, ‘has_media’, and ‘subreddit’ were transformed using StringIndexer. This step converted categorical strings into numerical indices, making them suitable for machine learning models.</p>
<p><code>One-Hot Encoding</code> The indexed categories were further encoded using OneHotEncoder. This process transformed the indexed categories into a binary vector representation, a necessary step for handling categorical data in many machine learning algorithms.</p>
<p><code>Vector Assembler</code> A VectorAssembler was utilized to combine various feature columns into a single vector column. This included both the newly created feature vectors from one-hot encoding and the existing numerical features such as ‘sentiment_score’, ‘num_comments’, ‘post_length’, ‘hour_of_day’, ‘day_of_week’, ‘day_of_month’, ‘month’, and ‘year’. This assembly created a unified feature vector essential for feeding into the machine learning models.</p>
<p><code>Pipeline Definition</code> A Pipeline was defined incorporating all the stages of string indexing, one-hot encoding, and vector assembly. This approach streamlined the preprocessing steps and ensured consistency across the data.</p>
<p><code>Pipeline Execution</code> The preprocessing pipeline was applied to the training and test data. This transformed the data according to the defined stages, ensuring that it was ready for model training and evaluation.</p>
</section>
<section id="model-training-and-evaluation-2" class="level3">
<h3 class="anchored" data-anchor-id="model-training-and-evaluation-2">Model Training and Evaluation</h3>
<section id="model-selection-2" class="level4">
<h4 class="anchored" data-anchor-id="model-selection-2">Model Selection</h4>
<p>For predicting Reddit post scores, we first used Linear Regression. This method is straightforward and good for starting out, as it helps us see how different things about a post might affect its popularity. But Reddit’s data can be quite complex, so we then switched to using a Decision Tree Regressor. This method is better at handling the tricky and detailed patterns found in social media data, like when posts are made and how people interact with them, giving us a clearer picture of what makes a post score high or low.</p>
</section>
<section id="model-training-2" class="level4">
<h4 class="anchored" data-anchor-id="model-training-2">Model Training</h4>
<p>In analyzing the performance of the two models employed for predicting Reddit post scores, Linear Regression and Decision Tree, distinct outcomes were observed. The Linear Regression model achieved a Root Mean Square Error (RMSE) of 1351.79 on training data and 1385.97 on testing data, with R-squared (R2) values of 0.25 and 0.3, respectively. These figures suggest moderate accuracy with a slightly better fit on the testing data. On the other hand, the Decision Tree model showed a slightly lower RMSE of 1324.6 in training, but a higher RMSE of 1447.39 in testing, indicating it was more accurate in training but less so with new data. The R2 values were 0.28 for training and 0.29 for testing, closely mirroring those of Linear Regression. Overall, while both models offered insights into the factors influencing post scores, their effectiveness was somewhat limited, pointing to the need for further refinement or exploration of more sophisticated models to enhance prediction accuracy.</p>
<p>These RMSE scores depict how good our model is. The lower the RMSE, the better the model is at predicting the score of a post. The RMSE for the Linear Regression model is 1351.79 on the training data and 1385.97 on the testing data. The RMSE for the Decision Tree model is 1324.6 on the training data and 1447.39 on the testing data. Such high RMSE does not always mean that the model is performing bad. The RMSE also depends on the range of the target variable. An model with RMSE of 1200 when the target variable range is 100-200 is a bad model. But an RMSE of 1200 when the target variable range is 10000-20000 is a good model. In our case, the target variable range is 0-20000. So, the RMSE of 1351.79 for Linear Regression and 1324.6 for Decision Tree are not bad.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/regression_plot.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure-7: Parity Plot for Decision Tree Regressor</figcaption><p></p>
</figure>
</div>
<!-- 
![Figure-3: ](img/Heatmap2_CM_Classification_Model1.png){width="100%" fig-align="center"}
![Figure-4: ](img/ROC2_Classification_Model1.png){width="100%" fig-align="center"} -->
<table class="table">
<thead>
<tr class="header">
<th>Model</th>
<th>RMSE Train</th>
<th>RMSE Test</th>
<th>R2 Train</th>
<th>R2 Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>1351.79</td>
<td>1385.97</td>
<td>0.25</td>
<td>0.28</td>
</tr>
<tr class="even">
<td>Decision Tree</td>
<td>1324.6</td>
<td>1447.39</td>
<td>0.28</td>
<td>0.22</td>
</tr>
</tbody>
</table>
<p>Trying to predict the scores of Reddit posts, which are calculated by subtracting downvotes from upvotes, turned out to be quite tough and the results weren’t as good as hoped. One big issue is that the same score can mean very different things. For example, a score of 1 could be because only one person liked the post and nobody disliked it, or it could be that 10,000 people liked it but 9,999 didn’t. This makes it really hard to predict what’s going on with a post just based on its score.</p>
<p>Also, a score doesn’t always tell the whole story about how users feel about a post. A post with lots of likes and dislikes probably means people have strong opinions about it, while a post with a few likes might have just not been seen by many people. This kind of difference is a big challenge for models like Linear Regression and Decision Trees. These models find it difficult to understand the real reasons behind a post’s score, especially because a score comes from both likes and dislikes. So, while these models can give some general ideas, they aren’t very good at accurately guessing a Reddit post’s score due to how complex and unclear the scoring can be.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Content 2023 by [Project Team 34] <br> All content licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</a></div>   
    <div class="nav-footer-right">Made with <a href="https://quarto.org/">Quarto</a><br> <a href="https://github.com/gu-dsan6000/fall-2023-reddit-project-team-34">View the source at GitHub</a></div>
  </div>
</footer>



</body></html>